{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ライブラリの読み込みなど\n",
    "\n",
    "# for train test\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import tensorflow.compat.v1 as tf\n",
    "import functools\n",
    "import json\n",
    "import enum\n",
    "\n",
    "# for make animation to mp4\n",
    "from matplotlib import tri as mtri\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions, provided in the release of the code from the original MeshGraphNets study:\n",
    "#https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "\n",
    "def triangles_to_edges(faces):\n",
    "  \"\"\"Computes mesh edges from triangles.\n",
    "     Note that this triangles_to_edges method was provided as part of the\n",
    "     code release for the MeshGraphNets paper by DeepMind, available here:\n",
    "     https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "  \"\"\"\n",
    "  # collect edges from triangles\n",
    "  edges = tf.concat([faces[:, 0:2],\n",
    "                     faces[:, 1:3],\n",
    "                     tf.stack([faces[:, 2], faces[:, 0]], axis=1)], axis=0)\n",
    "  # those edges are sometimes duplicated (within the mesh) and sometimes\n",
    "  # single (at the mesh boundary).\n",
    "  # sort & pack edges as single tf.int64\n",
    "  receivers = tf.reduce_min(edges, axis=1)\n",
    "  senders = tf.reduce_max(edges, axis=1)\n",
    "  packed_edges = tf.bitcast(tf.stack([senders, receivers], axis=1), tf.int64)\n",
    "  # remove duplicates and unpack\n",
    "  unique_edges = tf.bitcast(tf.unique(packed_edges)[0], tf.int32)\n",
    "  senders, receivers = tf.unstack(unique_edges, axis=1)\n",
    "  # create two-way connectivity\n",
    "  return (tf.concat([senders, receivers], axis=0),\n",
    "          tf.concat([receivers, senders], axis=0))\n",
    "\n",
    "class NodeType(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Define the code for the one-hot vector representing the node types.\n",
    "    Note that this is consistent with the codes provided in the original\n",
    "    MeshGraphNets study: \n",
    "    https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "    \"\"\"\n",
    "    NORMAL = 0\n",
    "    OBSTACLE = 1\n",
    "    AIRFOIL = 2\n",
    "    HANDLE = 3\n",
    "    INFLOW = 4\n",
    "    OUTFLOW = 5\n",
    "    WALL_BOUNDARY = 6\n",
    "    SIZE = 9\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## グローバル変数\n",
    "\n",
    "# flag\n",
    "use_processed_dataset = True # データの加工と保存はしときました．Trueで読み見込めます\n",
    "\n",
    "#純変数\n",
    "dt=0.01   #A constant: do not change!\n",
    "best_model_rollout_index = -1\n",
    "\n",
    "# 空リストなど\n",
    "best_model_rollout_data = [] # best modelの各ステップのデータを格納するvisualize時に呼び出す\n",
    "\n",
    "# pathの設定など1\n",
    "root_dir = '/root'\n",
    "dataset_dir = os.path.join(root_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/best_models')\n",
    "postprocess_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/animations')\n",
    "\n",
    "#pathの設定など2\n",
    "train_datafile_path = os.path.join(dataset_dir, 'train.h5')\n",
    "test_datafile_path = os.path.join(dataset_dir, 'test.h5')\n",
    "valid_datafile_path = os.path.join(dataset_dir, 'valid.h5')\n",
    "\n",
    "print(\"datafile directory: \" + dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの読み込み\n",
    "- train_datasets: トレーニング用のデータセットの集合\n",
    "- test_datasets: テスト用のデータセットの集合\n",
    "- val_datasets: validation用のデータセットの集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(data_file_path, tmax_raj=np.inf):\n",
    "    datasets = []\n",
    "    print('start reading ['+data_file_path+']')\n",
    "    \n",
    "    with h5py.File(data_file_path, 'r') as data:\n",
    "        for i,trajectory in enumerate(data.keys()):\n",
    "            print(\"Trajectory: \",i)\n",
    "            dataset = []\n",
    "            \n",
    "            for ts in range(len(data[trajectory]['velocity'])-1):\n",
    "                #Note that it's faster to convert to numpy then to torch than to\n",
    "                #import to torch from h5 format directly\n",
    "                momentum = torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "                #node_type = torch.tensor(np.array(data[trajectory]['node_type'][ts]))\n",
    "                node_type = torch.tensor(np.array(tf.one_hot(tf.convert_to_tensor(data[trajectory]['node_type'][0]), NodeType.SIZE))).squeeze(1)\n",
    "                x = torch.cat((momentum,node_type),dim=-1).type(torch.float)\n",
    "                \n",
    "                #Get edge indices in COO format\n",
    "                edges = triangles_to_edges(tf.convert_to_tensor(np.array(data[trajectory]['cells'][ts])))\n",
    "\n",
    "                edge_index = torch.cat( (torch.tensor(edges[0].numpy()).unsqueeze(0) ,\n",
    "                            torch.tensor(edges[1].numpy()).unsqueeze(0)), dim=0).type(torch.long)\n",
    "                \n",
    "                #Get edge features\n",
    "                u_i=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[0]]\n",
    "                u_j=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[1]]\n",
    "                u_ij=u_i-u_j\n",
    "                u_ij_norm = torch.norm(u_ij,p=2,dim=1,keepdim=True)\n",
    "                edge_attr = torch.cat((u_ij,u_ij_norm),dim=-1).type(torch.float)\n",
    "                \n",
    "                #Node outputs, for training (velocity)\n",
    "                v_t=torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "                v_tp1=torch.tensor(np.array(data[trajectory]['velocity'][ts+1]))\n",
    "                y=((v_tp1-v_t)/dt).type(torch.float)\n",
    "\n",
    "                #Node outputs, for testing integrator (pressure)\n",
    "                p=torch.tensor(np.array(data[trajectory]['pressure'][ts]))\n",
    "\n",
    "                #Data needed for visualization code\n",
    "                cells=torch.tensor(np.array(data[trajectory]['cells'][ts]))\n",
    "                mesh_pos=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))\n",
    "                \n",
    "                dataset.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=y,p=p,\n",
    "                                cells=cells,mesh_pos=mesh_pos))\n",
    "            \n",
    "            datasets.append(dataset)\n",
    "            if(i+1 == tmax_raj):\n",
    "                break\n",
    "    print('finish reading ['+data_file_path+']')\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(datasets, batch_size, shuffle, use_dataset_size=None):\n",
    "    data_list = []\n",
    "    current_dataset_size = 0\n",
    "    load_next = True\n",
    "    if use_dataset_size is None:\n",
    "        use_dataset_size = len(datasets) * len(datasets[0])\n",
    "        \n",
    "    for dataset in datasets:\n",
    "        for data in dataset:\n",
    "            data_list.append(data)\n",
    "            current_dataset_size+=1\n",
    "            if current_dataset_size == use_dataset_size:\n",
    "                load_next = False\n",
    "                break\n",
    "        if not load_next:\n",
    "            break\n",
    "    \n",
    "    loader = DataLoader(data_list, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetの準備or読み込み\n",
    "print('use_processed_dataset: {}'.format(use_processed_dataset))\n",
    "if use_processed_dataset:\n",
    "    # 読み込みに 5 分くらいかかる\n",
    "    print('start loading [train_datasets]')\n",
    "    train_datasets = torch.load(os.path.join(dataset_dir + '/train_processed_data_set.pt'))\n",
    "    print('start loading [test_datasets]')\n",
    "    test_datasets = torch.load(os.path.join(dataset_dir + '/test_processed_data_set.pt'))\n",
    "    print('start loading [valid_datasets]')\n",
    "    valid_datasets = torch.load(os.path.join(dataset_dir + '/valid_processed_data_set.pt'))\n",
    "    print('finish loading')\n",
    "\n",
    "else:    \n",
    "    train_datasets = make_datasets(train_datafile_path)\n",
    "    test_datasets = make_datasets(test_datafile_path)\n",
    "    valid_datasets = make_datasets(valid_datafile_path)\n",
    "    torch.save(train_datasets, os.path.join(dataset_dir + '/train_processed_data_set.pt'))\n",
    "    torch.save(test_datasets, os.path.join(dataset_dir + '/test_processed_data_set.pt'))\n",
    "    torch.save(valid_datasets, os.path.join(dataset_dir + '/valid_processed_data_set.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの扱いの参考\n",
    "print('- 訓練用データのトラジェクトリ数: {}'.format(len(train_datasets)))\n",
    "print('- 1トラジェクトリのタイムステップ数: {}'.format(len(train_datasets[0])))\n",
    "print('- 一ステップのデータのサイズ')\n",
    "print(train_datasets[0][0])\n",
    "print('- 一ステップのデータのx(全ノードのノード特徴量)の一例')\n",
    "print(train_datasets[0][0].x)\n",
    "print('- 一ステップのデータのxのあるノードの次元(x_axis_vero, y_axis_vero, (one_hot_vec size = 9))')\n",
    "print(len(train_datasets[0][0].x[0]))\n",
    "print('- 一ステップのデータのxのあるノードの一例')\n",
    "print(train_datasets[0][0].x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(to_normalize,mean_vec,std_vec):\n",
    "    return (to_normalize-mean_vec)/std_vec\n",
    "\n",
    "def unnormalize(to_unnormalize,mean_vec,std_vec):\n",
    "    return to_unnormalize*std_vec+mean_vec\n",
    "\n",
    "def get_stats(data_list):\n",
    "    '''\n",
    "    Method for normalizing processed datasets. Given  the processed data_list, \n",
    "    calculates the mean and standard deviation for the node features, edge features, \n",
    "    and node outputs, and normalizes these using the calculated statistics.\n",
    "    '''\n",
    "\n",
    "    #mean and std of the node features are calculated\n",
    "    mean_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "    std_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "\n",
    "    #mean and std of the edge features are calculated\n",
    "    mean_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "    std_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "\n",
    "    #mean and std of the output parameters are calculated\n",
    "    mean_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "    std_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "\n",
    "    #Define the maximum number of accumulations to perform such that we do\n",
    "    #not encounter memory issues\n",
    "    max_accumulations = 10**6\n",
    "\n",
    "    #Define a very small value for normalizing to \n",
    "    eps=torch.tensor(1e-8)\n",
    "\n",
    "    #Define counters used in normalization\n",
    "    num_accs_x = 0\n",
    "    num_accs_edge=0\n",
    "    num_accs_y=0\n",
    "\n",
    "    #Iterate through the data in the list to accumulate statistics\n",
    "    for dp in data_list:\n",
    "\n",
    "        #Add to the \n",
    "        mean_vec_x+=torch.sum(dp.x,dim=0)\n",
    "        std_vec_x+=torch.sum(dp.x**2,dim=0)\n",
    "        num_accs_x+=dp.x.shape[0]\n",
    "\n",
    "        mean_vec_edge+=torch.sum(dp.edge_attr,dim=0)\n",
    "        std_vec_edge+=torch.sum(dp.edge_attr**2,dim=0)\n",
    "        num_accs_edge+=dp.edge_attr.shape[0]\n",
    "\n",
    "        mean_vec_y+=torch.sum(dp.y,dim=0)\n",
    "        std_vec_y+=torch.sum(dp.y**2,dim=0)\n",
    "        num_accs_y+=dp.y.shape[0]\n",
    "\n",
    "        if(num_accs_x>max_accumulations or num_accs_edge>max_accumulations or num_accs_y>max_accumulations):\n",
    "            break\n",
    "\n",
    "    mean_vec_x = mean_vec_x/num_accs_x\n",
    "    std_vec_x = torch.maximum(torch.sqrt(std_vec_x/num_accs_x - mean_vec_x**2),eps)\n",
    "\n",
    "    mean_vec_edge = mean_vec_edge/num_accs_edge\n",
    "    std_vec_edge = torch.maximum(torch.sqrt(std_vec_edge/num_accs_edge - mean_vec_edge**2),eps)\n",
    "\n",
    "    mean_vec_y = mean_vec_y/num_accs_y\n",
    "    std_vec_y = torch.maximum(torch.sqrt(std_vec_y/num_accs_y - mean_vec_y**2),eps)\n",
    "\n",
    "    mean_std_list=[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y]\n",
    "\n",
    "    return mean_std_list\n",
    "\n",
    "def get_stats_from_datasets(datasets):\n",
    "    full_data_list = []\n",
    "    for dataset in datasets:\n",
    "        for data in dataset:\n",
    "            full_data_list.append(data)\n",
    "    stats_list = get_stats(full_data_list)\n",
    "    return stats_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,  **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(  **kwargs )\n",
    "        \"\"\"\n",
    "        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        reset parameters for stacked MLP layers\n",
    "        \"\"\"\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size = None):\n",
    "        \"\"\"\n",
    "        Handle the pre and post-processing of node features/embeddings,\n",
    "        as well as initiates message passing by calling the propagate function.\n",
    "\n",
    "        Note that message passing and aggregation are handled by the propagate\n",
    "        function, and the update\n",
    "\n",
    "        x has shpae [node_num , in_channels] (node embeddings)\n",
    "        edge_index: [2, edge_num]\n",
    "        edge_attr: [E, in_channels]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x = x, edge_attr = edge_attr, size = size) # out has the shape of [E, out_channels]\n",
    "\n",
    "        updated_nodes = torch.cat([x,out],dim=1)        # Complete the aggregation through self-aggregation\n",
    "\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        source_node: x_i has the shape of [E, in_channels]\n",
    "        target_node: x_j has the shape of [E, in_channels]\n",
    "        target_edge: edge_attr has the shape of [E, out_channels]\n",
    "\n",
    "        The messages that are passed are the raw embeddings. These are not processed.\n",
    "        \"\"\"\n",
    "\n",
    "        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]\n",
    "        updated_edges=self.edge_mlp(updated_edges)+edge_attr\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \"\"\"\n",
    "        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,\n",
    "        then we aggregate self message (from the edge itself). This is streamlined\n",
    "        into one operation here.\n",
    "        \"\"\"\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "        \"\"\"\n",
    "        MeshGraphNet model. This model is built upon Deepmind's 2021 paper.\n",
    "        This model consists of three parts: (1) Preprocessing: encoder (2) Processor\n",
    "        (3) postproccessing: decoder. Encoder has an edge and node decoders respectively.\n",
    "        Processor has two processors for edge and node respectively. Note that edge attributes have to be\n",
    "        updated first. Decoder is only for nodes.\n",
    "\n",
    "        Input_dim: dynamic variables + node_type + node_position\n",
    "        Hidden_dim: 128 in deepmind's paper\n",
    "        Output_dim: dynamic variables: velocity changes (1)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # encoder convert raw inputs into latent embeddings\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim))\n",
    "\n",
    "        self.edge_encoder = Sequential(Linear( input_dim_edge , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not >=1'\n",
    "\n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "\n",
    "        # decoder: only for node embeddings\n",
    "        self.decoder = Sequential(Linear( hidden_dim , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, output_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "\n",
    "    def forward(self,data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge):\n",
    "        \"\"\"\n",
    "        Encoder encodes graph (node/edge features) into latent vectors (node/edge embeddings)\n",
    "        The return of processor is fed into the processor for generating new feature vectors\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, pressure = data.x, data.edge_index, data.edge_attr, data.p\n",
    "\n",
    "        x = normalize(x,mean_vec_x,std_vec_x)\n",
    "        edge_attr=normalize(edge_attr,mean_vec_edge,std_vec_edge)\n",
    "\n",
    "        # Step 1: encode node/edge features into latent node/edge embeddings\n",
    "        x = self.node_encoder(x) # output shape is the specified hidden dimension\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr) # output shape is the specified hidden dimension\n",
    "\n",
    "        # step 2: perform message passing with latent node/edge embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x,edge_attr = self.processor[i](x,edge_index,edge_attr)\n",
    "\n",
    "        # step 3: decode latent node embeddings into physical quantities of interest\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def loss(self, pred, inputs,mean_vec_y,std_vec_y):\n",
    "        #Define the node types that we calculate loss for\n",
    "        normal=torch.tensor(NodeType.NORMAL)\n",
    "        outflow=torch.tensor(NodeType.OUTFLOW)\n",
    "\n",
    "        #Get the loss mask for the nodes of the types we calculate loss for\n",
    "        loss_mask=torch.logical_or((torch.argmax(inputs.x[:,2:],dim=1) == normal),\n",
    "                                   (torch.argmax(inputs.x[:,2:],dim=1) == outflow))\n",
    "\n",
    "        #Normalize labels with dataset statistics\n",
    "        labels = normalize(inputs.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "        #Find sum of square errors\n",
    "        error=torch.sum((labels-pred)**2,axis=1)\n",
    "\n",
    "        #Root and mean the errors for the nodes we calculate loss for\n",
    "        loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_loader(train_datasets, 2, shuffle=False)\n",
    "sample_data = next(iter(train_loader))\n",
    "print(sample_data.x.shape[1])\n",
    "print(sample_data.edge_attr.shape[1])\n",
    "print(sample_data.y.shape[1])\n",
    "# the dynamic variables have the shape of 2 (velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, train_stats_list, valid_stats_list, args):\n",
    "    '''\n",
    "    Performs a training loop on the dataset for MeshGraphNets. Also calls\n",
    "    test and validation functions.\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame(columns=['epoch', 'train_loss', 'test_loss', 'velo_val_loss'])\n",
    "\n",
    "    #Define the model name for saving \n",
    "    # model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "    #            '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "    #            '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "    model_name =    'model_nl'  +str(args.num_layers) + \\\n",
    "                    '_bs'       +str(args.batch_size) + \\\n",
    "                    '_hd'       +str(args.hidden_dim) + \\\n",
    "                    '_ep'       +str(args.epochs) + \\\n",
    "                    '_wd'       +str(args.weight_decay) + \\\n",
    "                    '_lr'       +str(args.lr) + \\\n",
    "                    '_shuff_'   +str(args.shuffle) + \\\n",
    "                    '_tr'       +str(args.train_size) + \\\n",
    "                    '_te'       +str(args.valid_size) ##_vaに変え忘れた\n",
    "\n",
    "    #torch_geometric DataLoaders are used for handling the data of lists of graphs\n",
    "    train_loader = make_loader(train_datasets, args.batch_size, shuffle=False, use_dataset_size=args.train_size)\n",
    "    valid_loader = make_loader(valid_datasets, args.batch_size, shuffle=False, use_dataset_size=args.valid_size)\n",
    "\n",
    "    #The statistics of the data are decomposed\n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = train_stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "        std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "\n",
    "    # build model\n",
    "    sample_data = next(iter(train_loader))\n",
    "    num_node_features = sample_data.x.shape[1] # 11(x dimension)\n",
    "    num_edge_features = sample_data.edge_attr.shape[1] # 3(edge_attr dimension)\n",
    "    num_classes = sample_data.y.shape[1] # 2(y dimension)\n",
    "\n",
    "    model = MeshGraphNet(input_dim_node=num_node_features,\n",
    "                         input_dim_edge=num_edge_features, \n",
    "                         hidden_dim=args.hidden_dim, \n",
    "                         output_dim=num_classes,\n",
    "                         args=args).to(device)\n",
    "    scheduler, opt = build_optimizer(args=args, params=model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    velo_val_losses = []\n",
    "    best_test_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    # stop=args.epochs: トレーニングのエポック数。これは訓練プロセス何エポック分実行するかを指定(5000?)\n",
    "    # desc=\"Training\": プログレスバーの表示名。ここでは \"Training\" と表示されます。\n",
    "    # unit=\"Epochs\": プログレスバーの単位。ここでは \"Epochs\" と表示されます。\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        num_loops=0\n",
    "        for batch in train_loader:\n",
    "            #Note that normalization must be done before it's called. The unnormalized\n",
    "            #data needs to be preserved in order to correctly calculate the loss\n",
    "            batch=batch.to(device)\n",
    "            opt.zero_grad()         #zero gradients each time\n",
    "            pred = model(batch,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            loss = model.loss(pred,batch,mean_vec_y,std_vec_y)\n",
    "            loss.backward()         #backpropagate loss\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            num_loops+=1\n",
    "        total_loss /= num_loops\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        #Every tenth epoch, calculate acceleration test loss and velocity validation loss\n",
    "        if epoch % 10 == 0:\n",
    "            if (args.save_velo_val):\n",
    "                # save velocity evaluation\n",
    "                # test_loss, velo_val_rmse = test(valid_loader,device,model,mean_vec_x,std_vec_x,mean_vec_edge,\n",
    "                #                  std_vec_edge,mean_vec_y,std_vec_y, args.save_velo_val)\n",
    "                test_loss, velo_val_rmse = test(valid_loader,\n",
    "                                                device,\n",
    "                                                model,\n",
    "                                                valid_stats_list, # valid用で良いのか，train用のを渡すのか...\n",
    "                                                args.save_velo_val)\n",
    "                velo_val_losses.append(velo_val_rmse.item())\n",
    "            else:\n",
    "                test_loss, _ = test(valid_loader,\n",
    "                                    device,\n",
    "                                    model,\n",
    "                                    valid_stats_list, # valid用で良いのか，train用のを渡すのか...\n",
    "                                    args.save_velo_val)\n",
    "\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # saving model\n",
    "            if not os.path.isdir( args.checkpoint_dir ):\n",
    "                os.mkdir(args.checkpoint_dir)\n",
    "\n",
    "            PATH = os.path.join(args.checkpoint_dir, model_name+'.csv')\n",
    "            df.to_csv(PATH,index=False)\n",
    "\n",
    "            #save the model if the current one is better than the previous best\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        else:\n",
    "            #If not the tenth epoch, append the previously calculated loss to the\n",
    "            #list in order to be able to plot it on the same plot as the training losses\n",
    "            if (args.save_velo_val):\n",
    "              test_losses.append(test_losses[-1])\n",
    "              velo_val_losses.append(velo_val_losses[-1])\n",
    "\n",
    "        if (args.save_velo_val):\n",
    "            # df = df.append({'epoch': epoch,'train_loss': losses[-1],\n",
    "            #                 'test_loss':test_losses[-1],\n",
    "            #                'velo_val_loss': velo_val_losses[-1]}, ignore_index=True)\n",
    "            new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                    'train_loss': [losses[-1]],\n",
    "                    'test_loss': [test_losses[-1]],\n",
    "                    'velo_val_loss': [velo_val_losses[-1]]})\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        else:\n",
    "            # df = df.append({'epoch': epoch, 'train_loss': losses[-1], 'test_loss': test_losses[-1]}, ignore_index=True)\n",
    "            new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                    'train_loss': [losses[-1]],\n",
    "                    'test_loss': [test_losses[-1]]})\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        # 100 Epoch ごとにモデルを保存しておく\n",
    "        if(epoch%100==0):\n",
    "            if (args.save_velo_val):\n",
    "                print(\"train loss\", str(round(total_loss, 2)),\n",
    "                      \"test loss\", str(round(test_loss.item(), 2)),\n",
    "                      \"velo loss\", str(round(velo_val_rmse.item(), 5)))\n",
    "            else:\n",
    "                print(\"train loss\", str(round(total_loss,2)), \"test loss\", str(round(test_loss.item(),2)))\n",
    "\n",
    "\n",
    "            if(args.save_best_model):\n",
    "\n",
    "                PATH = os.path.join(args.checkpoint_dir, model_name+'.pt')\n",
    "                torch.save(best_model.state_dict(), PATH )\n",
    "    \n",
    "    # valid_loader(もとは test_loader) を返す必要不明\n",
    "    return test_losses, losses, velo_val_losses, best_model, best_test_loss, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader,device,test_model,\n",
    "         stats_list, is_validation,\n",
    "          delta_t=0.01, save_model_preds=False, model_type=None):\n",
    "  \n",
    "    '''\n",
    "    Calculates test set losses and validation set errors.\n",
    "    '''\n",
    "\n",
    "    loss=0\n",
    "    velo_rmse = 0\n",
    "    num_loops=0\n",
    "    \n",
    "    [mean_vec_x, std_vec_x,\n",
    "     mean_vec_edge, std_vec_edge,\n",
    "     mean_vec_y, std_vec_y] = stats_list\n",
    "    (mean_vec_x, std_vec_x,\n",
    "     mean_vec_edge, std_vec_edge,\n",
    "     mean_vec_y, std_vec_y) = (mean_vec_x.to(device), std_vec_x.to(device),\n",
    "                               mean_vec_edge.to(device), std_vec_edge.to(device),\n",
    "                               mean_vec_y.to(device), std_vec_y.to(device))\n",
    "\n",
    "    for data in loader:\n",
    "        data=data.to(device)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #calculate the loss for the model given the test set\n",
    "            pred = test_model(data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            loss += test_model.loss(pred, data,mean_vec_y,std_vec_y)\n",
    "\n",
    "            #calculate validation error if asked to\n",
    "            if (is_validation):\n",
    "\n",
    "                #Like for the MeshGraphNets model, calculate the mask over which we calculate\n",
    "                #flow loss and add this calculated RMSE value to our val error\n",
    "                normal=torch.tensor(NodeType.NORMAL)\n",
    "                outflow=torch.tensor(NodeType.OUTFLOW)\n",
    "                loss_mask = torch.logical_or((torch.argmax(data.x[:, 2:], dim=1) == normal),\n",
    "                                             (torch.argmax(data.x[:, 2:], dim=1) == outflow))\n",
    "\n",
    "                eval_velo = data.x[:, 0:2] + unnormalize( pred[:], mean_vec_y, std_vec_y ) * delta_t\n",
    "                gs_velo = data.x[:, 0:2] + data.y[:] * delta_t\n",
    "                \n",
    "                error = torch.sum((eval_velo - gs_velo) ** 2, axis=1)\n",
    "                velo_rmse += torch.sqrt(torch.mean(error[loss_mask]))\n",
    "\n",
    "        num_loops+=1\n",
    "        # if velocity is evaluated, return velo_rmse as 0\n",
    "    return loss/num_loops, velo_rmse/num_loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで学習の準備 終了\n",
    "\n",
    "ここからモデルをトレーニングして，**最良のモデル**を探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in [\n",
    "        {'model_type': 'meshgraphnet',  \n",
    "         'num_layers': 10,\n",
    "         'batch_size': 2, # もとは16 \n",
    "         'hidden_dim': 10, \n",
    "         'epochs': 500,\n",
    "        #  'epochs': 5000,\n",
    "         'opt': 'adam', \n",
    "         'opt_scheduler': 'none', \n",
    "         'opt_restart': 0, \n",
    "         'weight_decay': 5e-4, \n",
    "         'lr': 0.001,\n",
    "         'train_size': 45, # 問題なく動けば len(train_datases)*len(train_datases[0]) \n",
    "        #  'test_size': 10, \n",
    "         'valid_size': 10, # 問題なく動けば len(valid_datases)*len(valid_datases[0]) \n",
    "         'device':'cuda',\n",
    "         'shuffle': True, \n",
    "         'save_velo_val': True,\n",
    "         'save_best_model': True, \n",
    "         'checkpoint_dir': './best_models/', # models/best_modelsとかのほうが分かりやすい？\n",
    "         'postprocess_dir': './2d_loss_plots/'},\n",
    "    ]:\n",
    "        args = objectview(args)\n",
    "        \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.device = device\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(5)  #Torch\n",
    "random.seed(5)        #Python\n",
    "np.random.seed(5)     #NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats_list = get_stats_from_datasets(train_datasets)\n",
    "valid_stats_list = get_stats_from_datasets(valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses, losses, velo_val_losses, best_model, best_test_loss, _ = train(device, train_stats_list, valid_stats_list, args)\n",
    "# test_losses, losses, velo_val_losses, best_model, best_test_loss, test_loader = train(device, train_stats_list, valid_stats_list, args)\n",
    "\n",
    "print(\"Min test set loss: {0}\".format(min(test_losses)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "if (args.save_velo_val):\n",
    "    print(\"Minimum velocity validation loss: {0}\".format(min(velo_val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(args, losses, test_losses, velo_val_losses):\n",
    "    # model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "    #            '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "    #            '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.valid_size)\n",
    "\n",
    "\n",
    "    if not os.path.isdir(args.postprocess_dir):\n",
    "        os.mkdir(args.postprocess_dir)\n",
    "\n",
    "    PATH = os.path.join(args.postprocess_dir, model_name + '.pdf')\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.title('Losses Plot')\n",
    "    plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "    plt.plot(test_losses, label=\"valid loss\" + \" - \" + args.model_type)\n",
    "    # plt.plot(test_losses, label=\"test loss\" + \" - \" + args.model_type)\n",
    "    #if (args.save_velo_val):\n",
    "    #    plt.plot(velo_val_losses, label=\"velocity loss\" + \" - \" + args.model_type)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    f.savefig(PATH, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots(args, losses, test_losses, velo_val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_test(args, test_stats_list, use_traj_size=None, delta_t=0.01, save_model_preds=False, model_type=None):\n",
    "\n",
    "    \n",
    "    if use_traj_size is None:\n",
    "        use_traj_size = len(test_datasets)\n",
    "    if use_traj_size > len(test_datasets):\n",
    "        print('use_traj_size is too big')\n",
    "        return\n",
    "    use_test_datasets = test_datasets[:use_traj_size]\n",
    "        \n",
    "    device = 'cpu'\n",
    "    # device = args.device\n",
    "    print('rollout device is {}'.format(device))\n",
    "    \n",
    "    #The statistics of the data are decomposed\n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = test_stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "        std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "    \n",
    "    rollout_traj_list = [] # 各要素がrollout_dataのリスト\n",
    "    rollout_loss_list = [] # 各要素が各トラジェクトリの全ステップのロスの平均\n",
    "    \n",
    "    sample_data = test_datasets[0][0]\n",
    "    \n",
    "    num_node_features = sample_data.x.shape[1] # 11(x dimension)\n",
    "    num_edge_features = sample_data.edge_attr.shape[1] # 3(edge_attr dimension)\n",
    "    num_classes = sample_data.y.shape[1] # 2(y dimension)\n",
    "    \n",
    "    best_model = MeshGraphNet(num_node_features,\n",
    "                         num_edge_features,\n",
    "                         args.hidden_dim,\n",
    "                         num_classes,\n",
    "                         args).to(device)\n",
    "    model_name ='model_nl'  +str(args.num_layers) + \\\n",
    "                '_bs'       +str(args.batch_size) + \\\n",
    "                '_hd'       +str(args.hidden_dim) + \\\n",
    "                '_ep'       +str(args.epochs) + \\\n",
    "                '_wd'       +str(args.weight_decay) + \\\n",
    "                '_lr'       +str(args.lr) + \\\n",
    "                '_shuff_'   +str(args.shuffle) + \\\n",
    "                '_tr'       +str(args.train_size) + \\\n",
    "                '_te'       +str(args.valid_size) ##_vaに変え忘れた\n",
    "    bese_model_filename = model_name + '.pt'\n",
    "    \n",
    "    checkpoint_dir='./best_models/' # argsのせいでこうなってる\n",
    "    PATH = os.path.join( checkpoint_dir, bese_model_filename)\n",
    "    best_model.load_state_dict(torch.load(PATH, map_location=args.device))\n",
    "    \n",
    "    for i, one_trajectory in enumerate(use_test_datasets):\n",
    "        one_trajectory_loss = 0\n",
    "        rollout_traj_x = []\n",
    "        \n",
    "        print('~~ start rollout No.{} ~~'.format(i))\n",
    "        for ts, one_step in enumerate(one_trajectory):\n",
    "            current_step = one_step\n",
    "            current_step = current_step.to(device)\n",
    "            if ts != 0:\n",
    "                current_step.x = pred_x\n",
    "            \n",
    "            pred_y = best_model(current_step,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            one_trajectory_loss += best_model.loss(pred_y, one_step,  mean_vec_y, std_vec_y)\n",
    "            pred_y = pred_y.to(device)\n",
    "            \n",
    "            padded_pred_y = torch.zeros((pred_y.size(0), num_node_features), device= device)\n",
    "            padded_pred_y[:, :2] = pred_y\n",
    "            \n",
    "            displace_x = padded_pred_y * delta_t\n",
    "            pred_x = current_step.x + displace_x\n",
    "            \n",
    "            rollout_traj_x.append(pred_x) # ts = 1~600になる．gsとかも予測(ts=1~600)でしてたから，そのまま読めばOK\n",
    "            \n",
    "        avg_one_trajectory_loss = one_trajectory_loss/len(one_trajectory)\n",
    "        rollout_traj_list.append(rollout_traj_x)\n",
    "        rollout_loss_list.append(avg_one_trajectory_loss)\n",
    "        print('   finish. loss avg={}'.format(avg_one_trajectory_loss))\n",
    "        \n",
    "    return rollout_traj_list, rollout_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_list = train_stats_list\n",
    "rollout_traj_list, rollout_loss_list = rollout_test(args, test_stats_list, use_traj_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rollout_traj_list について\n",
    "- rollout_traj_list[0 \\: max_traj] : ロールアウトのトラジェクトリの集合\n",
    "- rollout_traj_list[0] : トラジェクトリ0のロールアウトデータ(ts=1~600であることに注意)\n",
    "- rollout_traj_list[0][0] : トラジェクトリ0 のts=1の全ノードのデータ\n",
    "- rollout_traj_list[0][0][0][:2] : トラジェクトリ0 のts=1のノード0のデータの速度のx成分とy成分\n",
    "\n",
    "rollout_loss_listについて\n",
    "- rollout_loss_list[0 \\: max_traj] : ロールアウトの「トラジェクトリごとのロス平均」の集合\n",
    "- rollout_loss_list[0] : ロールアウトのトラジェクトリ0のロスの平均(ts=1~600のロスの平均)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アニメーションについて\n",
    "- とりあえず，ロスの平均が最小，中央，最大のものについてアニメーション化すると良い...？\n",
    "- 表示するものは colab と同様に gs, pred, errで良い？\n",
    "- min, medi, maxあたりで名前を変えてmp4で保存\n",
    "- 可能なら不要な部分切る"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
