{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeshGraphNets のPytorchによる実装\n",
    "b3 秋冬 ゼミナール\n",
    "中谷，山田\n",
    "\n",
    "参考\n",
    "- Midium 'Learning Mesh-Based Flow Simulations on Graph Networks' \n",
    "    <https://medium.com/stanford-cs224w/learning-mesh-based-flow-simulations-on-graph-networks-44983679cf2d>.\n",
    "\n",
    "- T. Pfaff et al., Learning Mesh-Based Simulation with Graph Networks (2021) ICLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9baf79843c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_scatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayerNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "# ライブラリの読み込みなど(実行は一回でOK)\n",
    "\n",
    "# for train test\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import tensorflow.compat.v1 as tf\n",
    "import functools\n",
    "import json\n",
    "import enum\n",
    "\n",
    "# for make animation to mp4\n",
    "from matplotlib import tri as mtri\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フラグ設定\n",
    "is_use_processed_data = False # False -> h5ファイルを読み込んでprocessから\n",
    "use_append_to_df = False # False -> connatを使用\n",
    "\n",
    "# パスの設定\n",
    "root_dir = '/root'\n",
    "dataset_dir = os.path.join(root_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/best_models')\n",
    "postprocess_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/animations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions, provided in the release of the code from the original MeshGraphNets study:\n",
    "#https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "\n",
    "def triangles_to_edges(faces):\n",
    "  \"\"\"Computes mesh edges from triangles.\n",
    "     Note that this triangles_to_edges method was provided as part of the\n",
    "     code release for the MeshGraphNets paper by DeepMind, available here:\n",
    "     https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "  \"\"\"\n",
    "  # collect edges from triangles\n",
    "  edges = tf.concat([faces[:, 0:2],\n",
    "                     faces[:, 1:3],\n",
    "                     tf.stack([faces[:, 2], faces[:, 0]], axis=1)], axis=0)\n",
    "  # those edges are sometimes duplicated (within the mesh) and sometimes\n",
    "  # single (at the mesh boundary).\n",
    "  # sort & pack edges as single tf.int64\n",
    "  receivers = tf.reduce_min(edges, axis=1)\n",
    "  senders = tf.reduce_max(edges, axis=1)\n",
    "  packed_edges = tf.bitcast(tf.stack([senders, receivers], axis=1), tf.int64)\n",
    "  # remove duplicates and unpack\n",
    "  unique_edges = tf.bitcast(tf.unique(packed_edges)[0], tf.int32)\n",
    "  senders, receivers = tf.unstack(unique_edges, axis=1)\n",
    "  # create two-way connectivity\n",
    "  return (tf.concat([senders, receivers], axis=0),\n",
    "          tf.concat([receivers, senders], axis=0))\n",
    "\n",
    "\n",
    "\n",
    "class NodeType(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Define the code for the one-hot vector representing the node types.\n",
    "    Note that this is consistent with the codes provided in the original\n",
    "    MeshGraphNets study: \n",
    "    https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "    \"\"\"\n",
    "    NORMAL = 0\n",
    "    OBSTACLE = 1\n",
    "    AIRFOIL = 2\n",
    "    HANDLE = 3\n",
    "    INFLOW = 4\n",
    "    OUTFLOW = 5\n",
    "    WALL_BOUNDARY = 6\n",
    "    SIZE = 9\n",
    "\n",
    "#Define the data folder and data file name\n",
    "datafile = os.path.join(dataset_dir, 'test.h5')\n",
    "print(\"path datafile : \" + datafile)\n",
    "data = h5py.File(datafile, 'r')\n",
    "file_path=os.path.join(dataset_dir, 'test_processed_set.pt')\n",
    "\n",
    "#Define the list that will return the data graphs\n",
    "data_list = []\n",
    "\n",
    "#define the time difference between the graphs\n",
    "dt=0.01   #A constant: do not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set の用意\n",
    "\n",
    "if not is_use_processed_data: ## not use preprocessed data\n",
    "    with h5py.File(datafile, 'r') as data:\n",
    "\n",
    "        for i,trajectory in enumerate(data.keys()):\n",
    "            # if(i==number_trajectories):\n",
    "            #     break\n",
    "            print(\"Trajectory: \",i)\n",
    "\n",
    "            #We iterate over all the time steps to produce an example graph except\n",
    "            #for the last one, which does not have a following time step to produce\n",
    "            #node output values\n",
    "            for ts in range(len(data[trajectory]['velocity'])-1):\n",
    "                # if(ts==number_ts):\n",
    "                #     break\n",
    "\n",
    "                #Get node features\n",
    "\n",
    "                #Note that it's faster to convert to numpy then to torch than to\n",
    "                #import to torch from h5 format directly\n",
    "                momentum = torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "                #node_type = torch.tensor(np.array(data[trajectory]['node_type'][ts]))\n",
    "                node_type = torch.tensor(np.array(tf.one_hot(tf.convert_to_tensor(data[trajectory]['node_type'][0]), NodeType.SIZE))).squeeze(1)\n",
    "                x = torch.cat((momentum,node_type),dim=-1).type(torch.float)\n",
    "\n",
    "                #Get edge indices in COO format\n",
    "                edges = triangles_to_edges(tf.convert_to_tensor(np.array(data[trajectory]['cells'][ts])))\n",
    "\n",
    "                edge_index = torch.cat( (torch.tensor(edges[0].numpy()).unsqueeze(0) ,\n",
    "                            torch.tensor(edges[1].numpy()).unsqueeze(0)), dim=0).type(torch.long)\n",
    "\n",
    "                #Get edge features\n",
    "                u_i=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[0]]\n",
    "                u_j=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[1]]\n",
    "                u_ij=u_i-u_j\n",
    "                u_ij_norm = torch.norm(u_ij,p=2,dim=1,keepdim=True)\n",
    "                edge_attr = torch.cat((u_ij,u_ij_norm),dim=-1).type(torch.float)\n",
    "\n",
    "                #Node outputs, for training (velocity)\n",
    "                v_t=torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "                v_tp1=torch.tensor(np.array(data[trajectory]['velocity'][ts+1]))\n",
    "                y=((v_tp1-v_t)/dt).type(torch.float)\n",
    "\n",
    "                #Node outputs, for testing integrator (pressure)\n",
    "                p=torch.tensor(np.array(data[trajectory]['pressure'][ts]))\n",
    "\n",
    "                #Data needed for visualization code\n",
    "                cells=torch.tensor(np.array(data[trajectory]['cells'][ts]))\n",
    "                mesh_pos=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))\n",
    "\n",
    "                data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=y,p=p,\n",
    "                                    cells=cells,mesh_pos=mesh_pos))\n",
    "\n",
    "    print(\"Done collecting data!\")\n",
    "\n",
    "    #os.path.join(data_folder + '/test.h5')\n",
    "    # 前処理したデータをdataset_dirに保存する処理？\n",
    "    torch.save(data_list, os.path.join(dataset_dir, 'test_processed_set.pt')) ## (謎のdata_folder)\n",
    "    #torch.save(data_list,'./'+dataset_dir+'/test_processed_set.pt')\n",
    "\n",
    "    print(\"Done saving data!\")\n",
    "    print(\"Output Location: \", dataset_dir+'/test_processed_set.pt')\n",
    "\n",
    "else: ## use preprocessed data\n",
    "    file_path=os.path.join(dataset_dir, 'test_processed_set.pt')\n",
    "    dataset_full_timesteps = torch.load(file_path)\n",
    "    dataset = torch.load(file_path)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化 メソッド\n",
    "\n",
    "def normalize(to_normalize,mean_vec,std_vec):\n",
    "    return (to_normalize-mean_vec)/std_vec\n",
    "\n",
    "def unnormalize(to_unnormalize,mean_vec,std_vec):\n",
    "    return to_unnormalize*std_vec+mean_vec\n",
    "\n",
    "def get_stats(data_list):\n",
    "    '''\n",
    "    Method for normalizing processed datasets. Given  the processed data_list, \n",
    "    calculates the mean and standard deviation for the node features, edge features, \n",
    "    and node outputs, and normalizes these using the calculated statistics.\n",
    "    '''\n",
    "\n",
    "    #mean and std of the node features are calculated\n",
    "    mean_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "    std_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "\n",
    "    #mean and std of the edge features are calculated\n",
    "    mean_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "    std_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "\n",
    "    #mean and std of the output parameters are calculated\n",
    "    mean_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "    std_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "\n",
    "    #Define the maximum number of accumulations to perform such that we do\n",
    "    #not encounter memory issues\n",
    "    max_accumulations = 10**6\n",
    "\n",
    "    #Define a very small value for normalizing to \n",
    "    eps=torch.tensor(1e-8)\n",
    "\n",
    "    #Define counters used in normalization\n",
    "    num_accs_x = 0\n",
    "    num_accs_edge=0\n",
    "    num_accs_y=0\n",
    "\n",
    "    #Iterate through the data in the list to accumulate statistics\n",
    "    for dp in data_list:\n",
    "\n",
    "        #Add to the \n",
    "        mean_vec_x+=torch.sum(dp.x,dim=0)\n",
    "        std_vec_x+=torch.sum(dp.x**2,dim=0)\n",
    "        num_accs_x+=dp.x.shape[0]\n",
    "\n",
    "        mean_vec_edge+=torch.sum(dp.edge_attr,dim=0)\n",
    "        std_vec_edge+=torch.sum(dp.edge_attr**2,dim=0)\n",
    "        num_accs_edge+=dp.edge_attr.shape[0]\n",
    "\n",
    "        mean_vec_y+=torch.sum(dp.y,dim=0)\n",
    "        std_vec_y+=torch.sum(dp.y**2,dim=0)\n",
    "        num_accs_y+=dp.y.shape[0]\n",
    "\n",
    "        if(num_accs_x>max_accumulations or num_accs_edge>max_accumulations or num_accs_y>max_accumulations):\n",
    "            break\n",
    "\n",
    "    mean_vec_x = mean_vec_x/num_accs_x\n",
    "    std_vec_x = torch.maximum(torch.sqrt(std_vec_x/num_accs_x - mean_vec_x**2),eps)\n",
    "\n",
    "    mean_vec_edge = mean_vec_edge/num_accs_edge\n",
    "    std_vec_edge = torch.maximum(torch.sqrt(std_vec_edge/num_accs_edge - mean_vec_edge**2),eps)\n",
    "\n",
    "    mean_vec_y = mean_vec_y/num_accs_y\n",
    "    std_vec_y = torch.maximum(torch.sqrt(std_vec_y/num_accs_y - mean_vec_y**2),eps)\n",
    "\n",
    "    mean_std_list=[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y]\n",
    "\n",
    "    return mean_std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークの定義\n",
    "以下の２つで定義\n",
    "- class MeshGraphNet(torch.nn.Module)\n",
    "    - __init__()でコンストラクタの動作を定義しており，ネットワークをインスタンス化(この中でProcessorLayerもインスタンス化)\n",
    "    - インスタンス名(必要な引数) で自動でforwardが呼ばれて計算される\n",
    "    - インスタンス名.loss() でロスを返す\n",
    "- class ProcessorLayer(MessagePassing)\n",
    "    - 頂点と辺それぞれのencode後のデータを受け取り，L回メッセージパスなどをして，decorderに渡す形式で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "        \"\"\"\n",
    "        MeshGraphNet model. This model is built upon Deepmind's 2021 paper.\n",
    "        This model consists of three parts: (1) Preprocessing: encoder (2) Processor\n",
    "        (3) postproccessing: decoder. Encoder has an edge and node decoders respectively.\n",
    "        Processor has two processors for edge and node respectively. Note that edge attributes have to be\n",
    "        updated first. Decoder is only for nodes.\n",
    "\n",
    "        Input_dim: dynamic variables + node_type + node_position\n",
    "        Hidden_dim: 128 in deepmind's paper\n",
    "        Output_dim: dynamic variables: velocity changes (1)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # encoder convert raw inputs into latent embeddings\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim))\n",
    "\n",
    "        self.edge_encoder = Sequential(Linear( input_dim_edge , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not >=1'\n",
    "\n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "\n",
    "        # decoder: only for node embeddings\n",
    "        self.decoder = Sequential(Linear( hidden_dim , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, output_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "\n",
    "    def forward(self,data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge):\n",
    "        \"\"\"\n",
    "        Encoder encodes graph (node/edge features) into latent vectors (node/edge embeddings)\n",
    "        The return of processor is fed into the processor for generating new feature vectors\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, pressure = data.x, data.edge_index, data.edge_attr, data.p\n",
    "\n",
    "        x = normalize(x,mean_vec_x,std_vec_x)\n",
    "        edge_attr=normalize(edge_attr,mean_vec_edge,std_vec_edge)\n",
    "\n",
    "        # Step 1: encode node/edge features into latent node/edge embeddings\n",
    "        x = self.node_encoder(x) # output shape is the specified hidden dimension\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr) # output shape is the specified hidden dimension\n",
    "\n",
    "        # step 2: perform message passing with latent node/edge embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x,edge_attr = self.processor[i](x,edge_index,edge_attr)\n",
    "\n",
    "        # step 3: decode latent node embeddings into physical quantities of interest\n",
    "\n",
    "        return self.decoder(x), x, edge_index, edge_attr\n",
    "\n",
    "    def loss(self, pred, inputs,mean_vec_y,std_vec_y):\n",
    "        #Define the node types that we calculate loss for\n",
    "        normal=torch.tensor(0)\n",
    "        outflow=torch.tensor(5)\n",
    "\n",
    "        #Get the loss mask for the nodes of the types we calculate loss for\n",
    "        loss_mask=torch.logical_or((torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(0)),\n",
    "                                   (torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(5)))\n",
    "\n",
    "        #Normalize labels with dataset statistics\n",
    "        labels = normalize(inputs.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "        #Find sum of square errors\n",
    "        error=torch.sum((labels-pred)**2,axis=1)\n",
    "\n",
    "        #Root and mean the errors for the nodes we calculate loss for\n",
    "        loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,  **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(  **kwargs )\n",
    "        \"\"\"\n",
    "        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        reset parameters for stacked MLP layers\n",
    "        \"\"\"\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size = None):\n",
    "        \"\"\"\n",
    "        Handle the pre and post-processing of node features/embeddings,\n",
    "        as well as initiates message passing by calling the propagate function.\n",
    "\n",
    "        Note that message passing and aggregation are handled by the propagate\n",
    "        function, and the update\n",
    "\n",
    "        x has shpae [node_num , in_channels] (node embeddings)\n",
    "        edge_index: [2, edge_num]\n",
    "        edge_attr: [E, in_channels]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x = x, edge_attr = edge_attr, size = size) # out has the shape of [E, out_channels]\n",
    "\n",
    "        updated_nodes = torch.cat([x,out],dim=1)        # Complete the aggregation through self-aggregation\n",
    "\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        source_node: x_i has the shape of [E, in_channels]\n",
    "        target_node: x_j has the shape of [E, in_channels]\n",
    "        target_edge: edge_attr has the shape of [E, out_channels]\n",
    "\n",
    "        The messages that are passed are the raw embeddings. These are not processed.\n",
    "        \"\"\"\n",
    "\n",
    "        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]\n",
    "        updated_edges=self.edge_mlp(updated_edges)+edge_attr\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \"\"\"\n",
    "        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,\n",
    "        then we aggregate self message (from the edge itself). This is streamlined\n",
    "        into one operation here.\n",
    "        \"\"\"\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化器(optimizer)の定義(実際はadamを使用する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adam_exp_decay':\n",
    "        # lr = tf.train.exponential_decay(learning_rate=1e-4,\n",
    "        #                           global_step=global_step, # global_step?? tf.train.exponential_decay??\n",
    "        #                           decay_steps=int(5e6),\n",
    "        #                           decay_rate=0.1) + 1e-6\n",
    "        # optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニング，テスト用のメソッド．\n",
    "\n",
    "ロールアウト実装のため返り値などをいじっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, device, stats_list, args):\n",
    "    '''\n",
    "    Performs a training loop on the dataset for MeshGraphNets. Also calls\n",
    "    test and validation functions.\n",
    "    '''\n",
    "    \n",
    "    model_rollout_data = []\n",
    "\n",
    "    df = pd.DataFrame(columns=['epoch','train_loss','test_loss', 'velo_val_loss'])\n",
    "\n",
    "    #Define the model name for saving \n",
    "    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "\n",
    "    #torch_geometric DataLoaders are used for handling the data of lists of graphs\n",
    "    loader = DataLoader(dataset[:args.train_size], batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset[args.train_size:], batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    #The statistics of the data are decomposed\n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "        std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "\n",
    "    # build model\n",
    "    num_node_features = dataset[0].x.shape[1]\n",
    "    num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "    num_classes = 2 # the dynamic variables have the shape of 2 (velocity)\n",
    "\n",
    "    model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, num_classes,\n",
    "                            args).to(device)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    velo_val_losses = []\n",
    "    best_test_loss = np.inf\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        num_loops=0\n",
    "        for batch in loader:\n",
    "            #Note that normalization must be done before it's called. The unnormalized\n",
    "            #data needs to be preserved in order to correctly calculate the loss\n",
    "            batch=batch.to(device)\n",
    "            opt.zero_grad()         #zero gradients each time\n",
    "            pred, _, _, _ = model(batch,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge) #edit yamada\n",
    "            loss = model.loss(pred,batch,mean_vec_y,std_vec_y)\n",
    "            loss.backward()         #backpropagate loss\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            num_loops+=1\n",
    "        total_loss /= num_loops\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        #Every tenth epoch, calculate acceleration test loss and velocity validation loss\n",
    "        if epoch % 10 == 0:\n",
    "            if (args.save_velo_val):\n",
    "                # save velocity evaluation\n",
    "                test_loss, velo_val_rmse, model_rollout_data = test(test_loader,device,model,mean_vec_x,std_vec_x,mean_vec_edge,\n",
    "                                 std_vec_edge,mean_vec_y,std_vec_y, args.save_velo_val) # edit yamada\n",
    "                velo_val_losses.append(velo_val_rmse.item())\n",
    "            else:\n",
    "                test_loss, _, model_rollout_data = test(test_loader,device,model,mean_vec_x,std_vec_x,mean_vec_edge,\n",
    "                                 std_vec_edge,mean_vec_y,std_vec_y, args.save_velo_val)\n",
    "\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # saving model\n",
    "            if not os.path.isdir( args.checkpoint_dir ):\n",
    "                os.mkdir(args.checkpoint_dir)\n",
    "\n",
    "            PATH = os.path.join(args.checkpoint_dir, model_name+'.csv')\n",
    "            df.to_csv(PATH,index=False)\n",
    "\n",
    "            #save the model if the current one is better than the previous best\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_model_rollout_data = model_rollout_data\n",
    "\n",
    "        else:\n",
    "            #If not the tenth epoch, append the previously calculated loss to the\n",
    "            #list in order to be able to plot it on the same plot as the training losses\n",
    "            if (args.save_velo_val):\n",
    "              test_losses.append(test_losses[-1])\n",
    "              velo_val_losses.append(velo_val_losses[-1])\n",
    "\n",
    "        if (args.save_velo_val):\n",
    "            if(use_append_to_df):\n",
    "                df = df.append({'epoch': epoch,'train_loss': losses[-1],\n",
    "                            'test_loss':test_losses[-1],\n",
    "                           'velo_val_loss': velo_val_losses[-1]}, ignore_index=True)\n",
    "            else: # add yamada\n",
    "                new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                        'train_loss': [losses[-1]],\n",
    "                        'test_loss': [test_losses[-1]],\n",
    "                        'velo_val_loss': [velo_val_losses[-1]]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "        else:\n",
    "            if(use_append_to_df):\n",
    "                df = df.append({'epoch': epoch, 'train_loss': losses[-1], 'test_loss': test_losses[-1]}, ignore_index=True)\n",
    "            else: # add yamada\n",
    "                new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                        'train_loss': [losses[-1]],\n",
    "                        'test_loss': [test_losses[-1]]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "                \n",
    "        if(epoch%100==0):\n",
    "            if (args.save_velo_val):\n",
    "                print(\"train loss\", str(round(total_loss, 2)),\n",
    "                      \"test loss\", str(round(test_loss.item(), 2)),\n",
    "                      \"velo loss\", str(round(velo_val_rmse.item(), 5)))\n",
    "            else:\n",
    "                print(\"train loss\", str(round(total_loss,2)), \"test loss\", str(round(test_loss.item(),2)))\n",
    "\n",
    "\n",
    "            if(args.save_best_model):\n",
    "\n",
    "                PATH = os.path.join(args.checkpoint_dir, model_name+'.pt')\n",
    "                torch.save(best_model.state_dict(), PATH )\n",
    "\n",
    "    return test_losses, losses, velo_val_losses, best_model, best_test_loss, test_loader\n",
    "\n",
    "\n",
    "def test(loader,device,test_model,\n",
    "         mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y, is_validation,\n",
    "          delta_t=0.01, save_model_preds=False, model_type=None):\n",
    "  \n",
    "    '''\n",
    "    Calculates test set losses and validation set errors.\n",
    "    '''\n",
    "\n",
    "    loss=0\n",
    "    velo_rmse = 0\n",
    "    num_loops=0\n",
    "    \n",
    "    prev_output = None #add yamada\n",
    "    model_rollout_data = [] # add yamada\n",
    "\n",
    "    for data in loader:\n",
    "        data=data.to(device)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #calculate the loss for the model given the test set\n",
    "            if prev_output is None:\n",
    "               pred, pred_x, pred_edge_index, pred_edge_attr = test_model(data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            else:\n",
    "               pred, pred_x, pred_edge_index, pred_edge_attr = test_model(prev_output, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge) #add yamada\n",
    "\n",
    "            loss += test_model.loss(pred, data,mean_vec_y,std_vec_y)\n",
    "            \n",
    "            data_dict = { #add yamada\n",
    "                'x': pred_x,\n",
    "                'edge_index': pred_edge_index, \n",
    "                'edge_attr': pred_edge_attr,\n",
    "                'y': pred,\n",
    "                'p': torch.randn(1923, 1),\n",
    "                'cells': torch.randn(3612, 3),\n",
    "                'mesh_pos': torch.randn(1923, 2)\n",
    "            } #add yamada\n",
    "            prev_output = Data(\n",
    "                x=data_dict['x'],\n",
    "                edge_index=data_dict['edge_index'],\n",
    "                edge_attr=data_dict['edge_attr'],\n",
    "                y=data_dict['y'],\n",
    "                p=data_dict['p'],\n",
    "                cells=data_dict['cells'],\n",
    "                mesh_pos=data_dict['mesh_pos']\n",
    "            )\n",
    "            \n",
    "            model_rollout_data.append(prev_output) # add yamada\n",
    "\n",
    "            #calculate validation error if asked to\n",
    "            if (is_validation):\n",
    "\n",
    "                #Like for the MeshGraphNets model, calculate the mask over which we calculate\n",
    "                #flow loss and add this calculated RMSE value to our val error\n",
    "                normal = torch.tensor(0)\n",
    "                outflow = torch.tensor(5)\n",
    "                loss_mask = torch.logical_or((torch.argmax(data.x[:, 2:], dim=1) == torch.tensor(0)),\n",
    "                                             (torch.argmax(data.x[:, 2:], dim=1) == torch.tensor(5)))\n",
    "\n",
    "                eval_velo = data.x[:, 0:2] + unnormalize( pred[:], mean_vec_y, std_vec_y ) * delta_t\n",
    "                gs_velo = data.x[:, 0:2] + data.y[:] * delta_t\n",
    "                \n",
    "                error = torch.sum((eval_velo - gs_velo) ** 2, axis=1)\n",
    "                velo_rmse += torch.sqrt(torch.mean(error[loss_mask]))\n",
    "\n",
    "        num_loops+=1\n",
    "        # if velocity is evaluated, return velo_rmse as 0\n",
    "    return loss/num_loops, velo_rmse/num_loops, model_rollout_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#与えられた辞書を引数として受け取り、その辞書の要素をクラスの属性として扱えるようにする？ \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロスの保存用のメソッド\n",
    "def save_plots(args, losses, test_losses, velo_val_losses):\n",
    "    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "\n",
    "    if not os.path.isdir(args.postprocess_dir):\n",
    "        os.mkdir(args.postprocess_dir)\n",
    "\n",
    "    PATH = os.path.join(args.postprocess_dir, model_name + '.pdf')\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.title('Losses Plot')\n",
    "    plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "    plt.plot(test_losses, label=\"test loss\" + \" - \" + args.model_type)\n",
    "    #if (args.save_velo_val):\n",
    "    #    plt.plot(velo_val_losses, label=\"velocity loss\" + \" - \" + args.model_type)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    f.savefig(PATH, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(args):\n",
    "    #To ensure reproducibility the best we can, here we control the sources of\n",
    "    #randomness by seeding the various random number generators used in this Colab\n",
    "    #For more information, see: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "    # シード値の固定(必要？？？)\n",
    "    torch.manual_seed(5)  #Torch\n",
    "    random.seed(5)        #Python\n",
    "    np.random.seed(5)     #NumPy\n",
    "\n",
    "    dataset = torch.load(file_path)[:(args.train_size+args.test_size)]\n",
    "\n",
    "    if(args.shuffle):\n",
    "        random.shuffle(dataset)\n",
    "\n",
    "    stats_list = get_stats(dataset)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    args.device = device\n",
    "    print(device)\n",
    "\n",
    "    test_losses, losses, velo_val_losses, best_model, best_test_loss, test_loader = train(dataset, device, stats_list, args)\n",
    "\n",
    "    print(\"Min test set loss: {0}\".format(min(test_losses)))\n",
    "    print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "    if (args.save_velo_val):\n",
    "        print(\"Minimum velocity validation loss: {0}\".format(min(velo_val_losses)))\n",
    "        \n",
    "    save_plots(args, losses, test_losses, velo_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アニメーション作成用のメソッド\n",
    "def make_animation(gs, pred, evl, path, name , skip = 2, save_anim = True, plot_variables = False):\n",
    "    '''\n",
    "    input gs is a dataloader and each entry contains attributes of many timesteps.\n",
    "\n",
    "    '''\n",
    "    print('Generating velocity fields...')\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(20, 16))\n",
    "    num_steps = len(gs) # for a single trajectory\n",
    "    num_frames = num_steps // skip\n",
    "    print(num_steps)\n",
    "    def animate(num):\n",
    "        step = (num*skip) % num_steps\n",
    "        traj = 0\n",
    "\n",
    "        bb_min = gs[0].x[:, 0:2].min() # first two columns are velocity\n",
    "        bb_max = gs[0].x[:, 0:2].max() # use max and min velocity of gs dataset at the first step for both \n",
    "                                          # gs and prediction plots\n",
    "        bb_min_evl = evl[0].x[:, 0:2].min()  # first two columns are velocity\n",
    "        bb_max_evl = evl[0].x[:, 0:2].max()  # use max and min velocity of gs dataset at the first step for both \n",
    "                                          # gs and prediction plots\n",
    "        count = 0\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.cla()\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_axis_off()\n",
    "            \n",
    "            pos = gs[step].mesh_pos \n",
    "            faces = gs[step].cells\n",
    "            if (count == 0):\n",
    "                # ground truth\n",
    "                velocity = gs[step].x[:, 0:2]\n",
    "                title = 'Ground truth:'\n",
    "            elif (count == 1):\n",
    "                velocity = pred[step].x[:, 0:2]\n",
    "                title = 'Prediction:'\n",
    "            else: \n",
    "                velocity = evl[step].x[:, 0:2]\n",
    "                title = 'Error: (Prediction - Ground truth)'\n",
    "\n",
    "            triang = mtri.Triangulation(pos[:, 0], pos[:, 1], faces)\n",
    "            if (count <= 1):\n",
    "                # absolute values\n",
    "                \n",
    "                mesh_plot = ax.tripcolor(triang, velocity[:, 0], vmin= bb_min, vmax=bb_max,  shading='flat' ) # x-velocity\n",
    "                ax.triplot(triang, 'ko-', ms=0.5, lw=0.3)\n",
    "            else:\n",
    "                # error: (pred - gs)/gs\n",
    "                mesh_plot = ax.tripcolor(triang, velocity[:, 0], vmin= bb_min_evl, vmax=bb_max_evl, shading='flat' ) # x-velocity\n",
    "                ax.triplot(triang, 'ko-', ms=0.5, lw=0.3)\n",
    "                #ax.triplot(triang, lw=0.5, color='0.5')\n",
    "\n",
    "            ax.set_title('{} Trajectory {} Step {}'.format(title, traj, step), fontsize = '20')\n",
    "            #ax.color\n",
    "\n",
    "            #if (count == 0):\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            clb = fig.colorbar(mesh_plot, cax=cax, orientation='vertical')\n",
    "            clb.ax.tick_params(labelsize=20) \n",
    "            \n",
    "            clb.ax.set_title('x velocity (m/s)',\n",
    "                             fontdict = {'fontsize': 20})\n",
    "            count += 1\n",
    "        return fig,\n",
    "\n",
    "    # Save animation for visualization\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if (save_anim):\n",
    "        gs_anim = animation.FuncAnimation(fig, animate, frames=num_frames, interval=1000)\n",
    "      #change from\n",
    "        # writergif = animation.PillowWriter(fps=10) \n",
    "        # anim_path = os.path.join(path, '{}_anim.gif'.format(name))\n",
    "        # gs_anim.save( anim_path, writer=writergif)\n",
    "        \n",
    "      #change to\n",
    "        anim_path = os.path.join(path, '{}_anim.mp4'.format(name))\n",
    "        gs_anim.save( anim_path, writer='ffmeg', fps=10)\n",
    "        \n",
    "        plt.show(block=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# アニメーション作成を呼び出すメソッド\n",
    "def visualize(loader, best_model, file_dir, args, gif_name, stats_list,\n",
    "              delta_t = 0.01, skip = 1):\n",
    "\n",
    "    best_model.eval()\n",
    "    device = args.device\n",
    "    viz_data = {}\n",
    "    gs_data = {}\n",
    "    eval_data = {}\n",
    "    viz_data_loader = copy.deepcopy(loader)\n",
    "    gs_data_loader = copy.deepcopy(loader)\n",
    "    eval_data_loader = copy.deepcopy(loader)\n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "            std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "    \n",
    "    \n",
    "    # flag = True #add yamada\n",
    "    # prev_pred_x = None\n",
    "    \n",
    "    for data, viz_data, gs_data, eval_data in zip(loader, viz_data_loader,\n",
    "                                                  gs_data_loader, eval_data_loader):\n",
    "        data=data.to(args.device) \n",
    "        viz_data = data.to(args.device)\n",
    "        \n",
    "        for data in best_model_rollout_data:\n",
    "            viz_data.x = data.x\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pred, _,  _, _ = best_model(data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            # pred gives the learnt accelaration between two timsteps\n",
    "            # next_vel = curr_vel + pred * delta_t  \n",
    "            \n",
    "            # edit yamada\n",
    "            # if flag: # 初回のみ実行\n",
    "            #     viz_data.x[:, 0:2] = data.x[:, 0:2] + pred[:]* delta_t\n",
    "            # else: # 二回目以降は前の予測を使って実行\n",
    "            #     viz_data.x[:, 0:2] = prev_pred_x[:] + pred[:]* delta_t\n",
    "            # prev_pred_x = viz_data.x[:, 0:2]\n",
    "           \n",
    "            \n",
    "            gs_data.x[:, 0:2] = data.x[:, 0:2] + data.y* delta_t\n",
    "            # gs_data - viz_data = error_data\n",
    "            eval_data.x[:, 0:2] = (viz_data.x[:, 0:2] - gs_data.x[:, 0:2])\n",
    "  \n",
    "    #print(viz_data_loader)\n",
    "    make_animation(gs_data_loader, viz_data_loader, eval_data_loader, file_dir,\n",
    "                      gif_name, skip, True, False)\n",
    "\n",
    "    return eval_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizeを呼び出すメソッド\n",
    "def do_visualize(args):\n",
    "    # load model. \n",
    "    args.device = torch.device('cpu') # animation function cannot work with data on GPU\n",
    "    num_node_features = dataset[0].x.shape[1]\n",
    "    num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "    num_classes = 2 # the dynamic variables have the shape of 2 (velocity)\n",
    "    PATH = os.path.join( checkpoint_dir, 'model_nl10_bs16_hd10_ep5000_wd0.0005_lr0.001_shuff_True_tr85_te15_new.pt')\n",
    "    model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, num_classes,\n",
    "                                args).to(args.device)\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH, map_location=args.device))\n",
    "\n",
    "    # visualize predicted velocities\n",
    "    animation_name = 'x_velocity'\n",
    "    args.postprocess_dir = postprocess_dir\n",
    "\n",
    "    eval_data_loader = visualize(dataset, model, args.postprocess_dir, args, animation_name, stats_list, \n",
    "            delta_t = 0.01, skip = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # **2回目以降はこの下から実行すればよい** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best modelのロールアウト時のデータであり，make_animationまで届ける必要があるためグローバル\n",
    "# 毎回リセットする必要あり\n",
    "best_model_rollout_data = [] # add yamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータなどの設定\n",
    "for args in [\n",
    "        {'model_type': 'meshgraphnet',\n",
    "         'num_layers': 10,\n",
    "         'batch_size': 16, \n",
    "         'hidden_dim': 10, \n",
    "         'epochs': 5000,\n",
    "         'opt': 'adam', \n",
    "         'opt_scheduler': 'none', \n",
    "         'opt_restart': 0, \n",
    "         'weight_decay': 5e-4, \n",
    "         'lr': 0.001,\n",
    "         'train_size': 3, \n",
    "         'test_size': 1, \n",
    "         'device':'cuda',\n",
    "         'shuffle': True, \n",
    "         'save_velo_val': True,\n",
    "         'save_best_model': True, \n",
    "         'checkpoint_dir': './best_models/',\n",
    "         'postprocess_dir': './2d_loss_plots/'},\n",
    "    ]:\n",
    "        args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 別の例\n",
    "# for args in [\n",
    "#         {'model_type': 'meshgraphnet',  \n",
    "#          'num_layers': 10,\n",
    "#          'batch_size': 16, \n",
    "#          'hidden_dim': 10, \n",
    "#          'epochs': 5000,\n",
    "#          'opt': 'adam', \n",
    "#          'opt_scheduler': 'none', \n",
    "#          'opt_restart': 0, \n",
    "#          'weight_decay': 5e-4, \n",
    "#          'lr': 0.001,\n",
    "#          'train_size': 45, #Train on 9 trajectories x 5 time steps\n",
    "#          'test_size': 10,  #Test on 10 time steps of two more trajectories\n",
    "#          'shuffle': False,       #Turn off shuffling so that data can be tested on completely unseen trajectories\n",
    "#          'save_velo_val': True,\n",
    "#          'save_best_model': True, \n",
    "#          'checkpoint_dir': './best_models/',\n",
    "#          'postprocess_dir': './2d_loss_plots/'},\n",
    "#     ]:\n",
    "#         args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上のセルで必要なargsを設定してから実行する．かなり時間がかかると思う．\n",
    "do_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上のセルを実行してbest_modelを見つけた後に，mp4ファイル作成のために実行する．かかる時間は不明．\n",
    "do_visualize(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_in_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
