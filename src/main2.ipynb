{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込みなど(実行は一回でOK)\n",
    "\n",
    "# for train test\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import tensorflow.compat.v1 as tf\n",
    "import functools\n",
    "import json\n",
    "import enum\n",
    "\n",
    "# for make animation to mp4\n",
    "from matplotlib import tri as mtri\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## グローバル変数\n",
    "#純変数\n",
    "dt=0.01   #A constant: do not change!\n",
    "best_model_rollout_index = -1\n",
    "# 空リストなど\n",
    "new_data_lists = [] # new_data_listはtime step(一つのシミュレーション)であり，それを順に入れているリストのリスト\n",
    "best_model_rollout_data = [] # best modelの各ステップのデータを格納するvisualize時に呼び出す\n",
    "# フラグ設定\n",
    "is_use_processed_data = False # False -> h5ファイルを読み込んでprocessから\n",
    "use_append_to_df = False # False -> connatを使用\n",
    "#pathの設定など1\n",
    "root_dir = '/root'\n",
    "dataset_dir = os.path.join(root_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/best_models')\n",
    "postprocess_dir = os.path.join(root_dir, 'bachlor3-meshgraphnets/animations')\n",
    "#pathの設定など2\n",
    "datafile = os.path.join(dataset_dir, 'test.h5')\n",
    "print(\"path datafile : \" + datafile)\n",
    "data = h5py.File(datafile, 'r')\n",
    "file_path=os.path.join(dataset_dir, 'test_processed_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions, provided in the release of the code from the original MeshGraphNets study:\n",
    "#https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "\n",
    "def triangles_to_edges(faces):\n",
    "  \"\"\"Computes mesh edges from triangles.\n",
    "     Note that this triangles_to_edges method was provided as part of the\n",
    "     code release for the MeshGraphNets paper by DeepMind, available here:\n",
    "     https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "  \"\"\"\n",
    "  # collect edges from triangles\n",
    "  edges = tf.concat([faces[:, 0:2],\n",
    "                     faces[:, 1:3],\n",
    "                     tf.stack([faces[:, 2], faces[:, 0]], axis=1)], axis=0)\n",
    "  # those edges are sometimes duplicated (within the mesh) and sometimes\n",
    "  # single (at the mesh boundary).\n",
    "  # sort & pack edges as single tf.int64\n",
    "  receivers = tf.reduce_min(edges, axis=1)\n",
    "  senders = tf.reduce_max(edges, axis=1)\n",
    "  packed_edges = tf.bitcast(tf.stack([senders, receivers], axis=1), tf.int64)\n",
    "  # remove duplicates and unpack\n",
    "  unique_edges = tf.bitcast(tf.unique(packed_edges)[0], tf.int32)\n",
    "  senders, receivers = tf.unstack(unique_edges, axis=1)\n",
    "  # create two-way connectivity\n",
    "  return (tf.concat([senders, receivers], axis=0),\n",
    "          tf.concat([receivers, senders], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeType(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Define the code for the one-hot vector representing the node types.\n",
    "    Note that this is consistent with the codes provided in the original\n",
    "    MeshGraphNets study: \n",
    "    https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "    \"\"\"\n",
    "    NORMAL = 0\n",
    "    OBSTACLE = 1\n",
    "    AIRFOIL = 2\n",
    "    HANDLE = 3\n",
    "    INFLOW = 4\n",
    "    OUTFLOW = 5\n",
    "    WALL_BOUNDARY = 6\n",
    "    SIZE = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set の用意\n",
    "with h5py.File(datafile, 'r') as data:\n",
    "\n",
    "    for i,trajectory in enumerate(data.keys()):\n",
    "        # if(i==number_trajectories):\n",
    "        #     break\n",
    "        print(\"Trajectory: \",i)\n",
    "        \n",
    "        new_data_list = []\n",
    "\n",
    "        #We iterate over all the time steps to produce an example graph except\n",
    "        #for the last one, which does not have a following time step to produce\n",
    "        #node output values\n",
    "        for ts in range(len(data[trajectory]['velocity'])-1):\n",
    "\n",
    "            #Get node features\n",
    "\n",
    "            #Note that it's faster to convert to numpy then to torch than to\n",
    "            #import to torch from h5 format directly\n",
    "            momentum = torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "            #node_type = torch.tensor(np.array(data[trajectory]['node_type'][ts]))\n",
    "            node_type = torch.tensor(np.array(tf.one_hot(tf.convert_to_tensor(data[trajectory]['node_type'][0]), NodeType.SIZE))).squeeze(1)\n",
    "            x = torch.cat((momentum,node_type),dim=-1).type(torch.float)\n",
    "\n",
    "            #Get edge indices in COO format\n",
    "            edges = triangles_to_edges(tf.convert_to_tensor(np.array(data[trajectory]['cells'][ts])))\n",
    "\n",
    "            edge_index = torch.cat( (torch.tensor(edges[0].numpy()).unsqueeze(0) ,\n",
    "                        torch.tensor(edges[1].numpy()).unsqueeze(0)), dim=0).type(torch.long)\n",
    "\n",
    "            #Get edge features\n",
    "            u_i=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[0]]\n",
    "            u_j=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))[edge_index[1]]\n",
    "            u_ij=u_i-u_j\n",
    "            u_ij_norm = torch.norm(u_ij,p=2,dim=1,keepdim=True)\n",
    "            edge_attr = torch.cat((u_ij,u_ij_norm),dim=-1).type(torch.float)\n",
    "\n",
    "            #Node outputs, for training (velocity)\n",
    "            v_t=torch.tensor(np.array(data[trajectory]['velocity'][ts]))\n",
    "            v_tp1=torch.tensor(np.array(data[trajectory]['velocity'][ts+1]))\n",
    "            y=((v_tp1-v_t)/dt).type(torch.float)\n",
    "\n",
    "            #Node outputs, for testing integrator (pressure)\n",
    "            p=torch.tensor(np.array(data[trajectory]['pressure'][ts]))\n",
    "\n",
    "            #Data needed for visualization code\n",
    "            cells=torch.tensor(np.array(data[trajectory]['cells'][ts]))\n",
    "            mesh_pos=torch.tensor(np.array(data[trajectory]['mesh_pos'][ts]))\n",
    "\n",
    "            # data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=y,p=p,\n",
    "            #                     cells=cells,mesh_pos=mesh_pos))\n",
    "            new_data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=y,p=p,\n",
    "                                cells=cells,mesh_pos=mesh_pos))\n",
    "            \n",
    "        new_data_lists.append(new_data_list)\n",
    "\n",
    "print(\"Done collecting data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_data_lists[5][0].x[:, 2:][0:10])\n",
    "print(new_data_lists[5][0].x[:10, 2:])\n",
    "# print(new_data_lists[5][0].x[:, 2:])\n",
    "print(len(new_data_lists[5][0].x[:10, 2:]))\n",
    "print(len(new_data_lists[5][0].x[:10, 2:][0]))\n",
    "\n",
    "print(torch.argmax(new_data_lists[5][0].x[:10, 2:], dim=0))\n",
    "print(torch.argmax(new_data_lists[5][0].x[:10, 2:], dim=1))\n",
    "\n",
    "\n",
    "# print(len(new_data_lists))\n",
    "# print(len(new_data_lists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(to_normalize,mean_vec,std_vec):\n",
    "    return (to_normalize-mean_vec)/std_vec\n",
    "\n",
    "def unnormalize(to_unnormalize,mean_vec,std_vec):\n",
    "    return to_unnormalize*std_vec+mean_vec\n",
    "\n",
    "def get_stats(data_list):\n",
    "    '''\n",
    "    Method for normalizing processed datasets. Given  the processed data_list, \n",
    "    calculates the mean and standard deviation for the node features, edge features, \n",
    "    and node outputs, and normalizes these using the calculated statistics.\n",
    "    '''\n",
    "\n",
    "    #mean and std of the node features are calculated\n",
    "    mean_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "    std_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "\n",
    "    #mean and std of the edge features are calculated\n",
    "    mean_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "    std_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "\n",
    "    #mean and std of the output parameters are calculated\n",
    "    mean_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "    std_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "\n",
    "    #Define the maximum number of accumulations to perform such that we do\n",
    "    #not encounter memory issues\n",
    "    max_accumulations = 10**6\n",
    "\n",
    "    #Define a very small value for normalizing to \n",
    "    eps=torch.tensor(1e-8)\n",
    "\n",
    "    #Define counters used in normalization\n",
    "    num_accs_x = 0\n",
    "    num_accs_edge=0\n",
    "    num_accs_y=0\n",
    "\n",
    "    #Iterate through the data in the list to accumulate statistics\n",
    "    for dp in data_list:\n",
    "\n",
    "        #Add to the \n",
    "        mean_vec_x+=torch.sum(dp.x,dim=0)\n",
    "        std_vec_x+=torch.sum(dp.x**2,dim=0)\n",
    "        num_accs_x+=dp.x.shape[0]\n",
    "\n",
    "        mean_vec_edge+=torch.sum(dp.edge_attr,dim=0)\n",
    "        std_vec_edge+=torch.sum(dp.edge_attr**2,dim=0)\n",
    "        num_accs_edge+=dp.edge_attr.shape[0]\n",
    "\n",
    "        mean_vec_y+=torch.sum(dp.y,dim=0)\n",
    "        std_vec_y+=torch.sum(dp.y**2,dim=0)\n",
    "        num_accs_y+=dp.y.shape[0]\n",
    "\n",
    "        if(num_accs_x>max_accumulations or num_accs_edge>max_accumulations or num_accs_y>max_accumulations):\n",
    "            break\n",
    "\n",
    "    mean_vec_x = mean_vec_x/num_accs_x\n",
    "    std_vec_x = torch.maximum(torch.sqrt(std_vec_x/num_accs_x - mean_vec_x**2),eps)\n",
    "\n",
    "    mean_vec_edge = mean_vec_edge/num_accs_edge\n",
    "    std_vec_edge = torch.maximum(torch.sqrt(std_vec_edge/num_accs_edge - mean_vec_edge**2),eps)\n",
    "\n",
    "    mean_vec_y = mean_vec_y/num_accs_y\n",
    "    std_vec_y = torch.maximum(torch.sqrt(std_vec_y/num_accs_y - mean_vec_y**2),eps)\n",
    "\n",
    "    mean_std_list=[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y]\n",
    "\n",
    "    return mean_std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "        \"\"\"\n",
    "        MeshGraphNet model. This model is built upon Deepmind's 2021 paper.\n",
    "        This model consists of three parts: (1) Preprocessing: encoder (2) Processor\n",
    "        (3) postproccessing: decoder. Encoder has an edge and node decoders respectively.\n",
    "        Processor has two processors for edge and node respectively. Note that edge attributes have to be\n",
    "        updated first. Decoder is only for nodes.\n",
    "\n",
    "        Input_dim: dynamic variables + node_type + node_position\n",
    "        Hidden_dim: 128 in deepmind's paper\n",
    "        Output_dim: dynamic variables: velocity changes (1)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # encoder convert raw inputs into latent embeddings\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim))\n",
    "\n",
    "        self.edge_encoder = Sequential(Linear( input_dim_edge , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not >=1'\n",
    "\n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "\n",
    "        # decoder: only for node embeddings\n",
    "        self.decoder = Sequential(Linear( hidden_dim , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, output_dim)\n",
    "                              )\n",
    "        \n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "\n",
    "    def forward(self,data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge):\n",
    "        \"\"\"\n",
    "        Encoder encodes graph (node/edge features) into latent vectors (node/edge embeddings)\n",
    "        The return of processor is fed into the processor for generating new feature vectors\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, pressure = data.x, data.edge_index, data.edge_attr, data.p\n",
    "\n",
    "        x = normalize(x,mean_vec_x,std_vec_x)\n",
    "        edge_attr=normalize(edge_attr,mean_vec_edge,std_vec_edge)\n",
    "\n",
    "        # Step 1: encode node/edge features into latent node/edge embeddings\n",
    "        x = self.node_encoder(x) # output shape is the specified hidden dimension\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr) # output shape is the specified hidden dimension\n",
    "\n",
    "        # step 2: perform message passing with latent node/edge embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x,edge_attr = self.processor[i](x,edge_index,edge_attr)\n",
    "\n",
    "        # step 3: decode latent node embeddings into physical quantities of interest\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def loss(self, pred, inputs,mean_vec_y,std_vec_y):\n",
    "        #Define the node types that we calculate loss for\n",
    "        normal=torch.tensor(0)\n",
    "        outflow=torch.tensor(5)\n",
    "\n",
    "        #Get the loss mask for the nodes of the types we calculate loss for\n",
    "        loss_mask=torch.logical_or((torch.argmax(inputs.x[:, 2:],dim=1)==normal),\n",
    "                                   (torch.argmax(inputs.x[:, 2:],dim=1)==outflow))\n",
    "        \n",
    "        #Normalize labels with dataset statistics\n",
    "        labels = normalize(inputs.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "        #Find sum of square errors\n",
    "        error=torch.sum((labels-pred)**2,axis=1)\n",
    "\n",
    "        #Root and mean the errors for the nodes we calculate loss for\n",
    "        loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def one_step_loss(self, pred, one_step_input ,mean_vec_y,std_vec_y):\n",
    "        #Define the node types that we calculate loss for\n",
    "        fluid_node_type=torch.tensor(0)\n",
    "        outflow_node_type=torch.tensor(5)\n",
    "\n",
    "        #Get the loss mask for the nodes of the types we calculate loss for\n",
    "        # x[2:11]はノードタイプで，fluid_node_typeなら0番目の要素が一のワンホットベクトル，outflow_node_typeなら5\n",
    "        # これを用いて，今回のノード数をnとして，上記どちらかのノードタイプならTrue，他はFalseのリストを作っておく(他のノードタイプは学習しない)\n",
    "        loss_mask=torch.logical_or((torch.argmax(one_step_input.x[:, 2:],dim=1)==fluid_node_type),\n",
    "                                   (torch.argmax(one_step_input.x[:, 2:],dim=1)==outflow_node_type))\n",
    "        \n",
    "        \n",
    "        #Normalize labels with dataset statistics\n",
    "        labels = normalize(one_step_input.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "        #Find sum of square errors\n",
    "        # dim = 1 にすることで，各行が誤差の二乗和(ノードの速度のx成分のの誤差の二乗 + ノードの速度のy成分のの誤差の二乗)となる\n",
    "        error=torch.sum((labels-pred)**2, dim=1)\n",
    "\n",
    "        #Root and mean the errors for the nodes we calculate loss for\n",
    "        loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,  **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(  **kwargs )\n",
    "        \"\"\"\n",
    "        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        reset parameters for stacked MLP layers\n",
    "        \"\"\"\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size = None):\n",
    "        \"\"\"\n",
    "        Handle the pre and post-processing of node features/embeddings,\n",
    "        as well as initiates message passing by calling the propagate function.\n",
    "\n",
    "        Note that message passing and aggregation are handled by the propagate\n",
    "        function, and the update\n",
    "\n",
    "        x has shpae [node_num , in_channels] (node embeddings)\n",
    "        edge_index: [2, edge_num]\n",
    "        edge_attr: [E, in_channels]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x = x, edge_attr = edge_attr, size = size) # out has the shape of [E, out_channels]\n",
    "\n",
    "        updated_nodes = torch.cat([x,out],dim=1)        # Complete the aggregation through self-aggregation\n",
    "\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        source_node: x_i has the shape of [E, in_channels]\n",
    "        target_node: x_j has the shape of [E, in_channels]\n",
    "        target_edge: edge_attr has the shape of [E, out_channels]\n",
    "\n",
    "        The messages that are passed are the raw embeddings. These are not processed.\n",
    "        \"\"\"\n",
    "\n",
    "        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]\n",
    "        updated_edges=self.edge_mlp(updated_edges)+edge_attr\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \"\"\"\n",
    "        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,\n",
    "        then we aggregate self message (from the edge itself). This is streamlined\n",
    "        into one operation here.\n",
    "        \"\"\"\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(arg_train_dataset, arg_test_data_lists, device, stats_list, args):\n",
    "    '''\n",
    "    Performs a training loop on the dataset for MeshGraphNets. Also calls\n",
    "    test and validation functions.\n",
    "    '''\n",
    "    \n",
    "    model_rollout_data_list = [] \n",
    "\n",
    "    df = pd.DataFrame(columns=['epoch','train_loss','test_loss', 'velo_val_loss'])\n",
    "\n",
    "    #保存するモデル名(argsに対応して決めている．test_trajとtrain_trajに変えてる)\n",
    "    # model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "    #            '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "    #            '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_traj)+'_te'+str(args.test_traj)\n",
    "               \n",
    "    ## データローダー　※かなり変更してる(batch_size も 2 にした． シャッフルは先にしているためFalse)\n",
    "    train_loader = DataLoader(arg_train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    # テスト用のデータはシミュレーションリストずつ渡す shape = (test_traj, max_ts)\n",
    "    test_data_lists = arg_test_data_lists\n",
    "    \n",
    "    #The statistics of the data are decomposed\n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "        std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "\n",
    "    # build model\n",
    "    num_node_features = arg_train_dataset[0].x.shape[1]\n",
    "    num_edge_features = arg_train_dataset[0].edge_attr.shape[1]\n",
    "    num_classes = 2 # the dynamic variables have the shape of 2 (velocity)\n",
    "\n",
    "    model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, num_classes, args).to(device)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    velo_val_losses = []\n",
    "    best_test_loss = np.inf\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        num_loops=0\n",
    "        for batch in train_loader:\n",
    "            #Note that normalization must be done before it's called. The unnormalized\n",
    "            #data needs to be preserved in order to correctly calculate the loss\n",
    "            batch=batch.to(device)\n",
    "            opt.zero_grad()         #zero gradients each time\n",
    "            pred = model(batch,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge) #edit yamada\n",
    "            loss = model.loss(pred,batch,mean_vec_y,std_vec_y)\n",
    "            loss.backward()         #backpropagate loss\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            num_loops+=1\n",
    "        total_loss /= num_loops\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        #Every tenth epoch, calculate acceleration test loss and velocity validation loss\n",
    "        if epoch % 10 == 0:\n",
    "            for simulation_num, test_data_list in enumerate(test_data_lists):\n",
    "                print(' ~~test simulation No.{}~~~'.format(simulation_num))\n",
    "                if (args.save_velo_val):\n",
    "                    test_loss, velo_val_rmse, model_rollout_data_list = test(test_data_list ,device,model,mean_vec_x,std_vec_x,mean_vec_edge,\n",
    "                                    std_vec_edge,mean_vec_y,std_vec_y, args.save_velo_val)\n",
    "                    velo_val_losses.append(velo_val_rmse.item())\n",
    "                else:\n",
    "                    test_loss, _, model_rollout_data_list = test(test_data_list ,device,model,mean_vec_x,std_vec_x,mean_vec_edge,\n",
    "                                    std_vec_edge,mean_vec_y,std_vec_y, args.save_velo_val)\n",
    "                \n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "                # saving model\n",
    "                if not os.path.isdir( args.checkpoint_dir ):\n",
    "                    os.mkdir(args.checkpoint_dir)\n",
    "\n",
    "                PATH = os.path.join(args.checkpoint_dir, model_name+'.csv')\n",
    "                df.to_csv(PATH,index=False)\n",
    "\n",
    "                #save the model if the current one is better than the previous best\n",
    "                if test_loss < best_test_loss:\n",
    "                    best_test_loss = test_loss\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_model_rollout_data = model_rollout_data_list\n",
    "                    best_model_rollout_index = simulation_num\n",
    "\n",
    "        else:\n",
    "            #If not the tenth epoch, append the previously calculated loss to the\n",
    "            #list in order to be able to plot it on the same plot as the training losses\n",
    "            if (args.save_velo_val):\n",
    "                test_losses.append(test_losses[-1])\n",
    "                velo_val_losses.append(velo_val_losses[-1])\n",
    "\n",
    "        if (args.save_velo_val):\n",
    "            if(use_append_to_df):\n",
    "                df = df.append({'epoch': epoch,'train_loss': losses[-1],\n",
    "                            'test_loss':test_losses[-1],\n",
    "                        'velo_val_loss': velo_val_losses[-1]}, ignore_index=True)\n",
    "            else: # add yamada\n",
    "                new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                        'train_loss': [losses[-1]],\n",
    "                        'test_loss': [test_losses[-1]],\n",
    "                        'velo_val_loss': [velo_val_losses[-1]]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "        else:\n",
    "            if(use_append_to_df):\n",
    "                df = df.append({'epoch': epoch, 'train_loss': losses[-1], 'test_loss': test_losses[-1]}, ignore_index=True)\n",
    "            else: # add yamada\n",
    "                new_row = pd.DataFrame({'epoch': [epoch],\n",
    "                        'train_loss': [losses[-1]],\n",
    "                        'test_loss': [test_losses[-1]]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "            \n",
    "        if(epoch%100==0):\n",
    "            if (args.save_velo_val):\n",
    "                print(\"train loss\", str(round(total_loss, 2)),\n",
    "                    \"test loss\", str(round(test_loss.item(), 2)),\n",
    "                    \"velo loss\", str(round(velo_val_rmse.item(), 5)))\n",
    "            else:\n",
    "                print(\"train loss\", str(round(total_loss,2)), \"test loss\", str(round(test_loss.item(),2)))\n",
    "\n",
    "            if(args.save_best_model):\n",
    "                PATH = os.path.join(args.checkpoint_dir, model_name+'.pt')\n",
    "                torch.save(best_model.state_dict(), PATH )\n",
    "\n",
    "    return test_losses, losses, velo_val_losses, best_model, best_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_list ,device,test_model,\n",
    "         mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y, is_validation,\n",
    "          delta_t=0.01, save_model_preds=False, model_type=None):\n",
    "  \n",
    "    '''\n",
    "    Calculates test set losses and validation set errors.\n",
    "    '''\n",
    "\n",
    "    one_simulation_loss=0\n",
    "    velo_rmse = 0\n",
    "    num_loops=0\n",
    "    \n",
    "    prev_step = None #add yamada\n",
    "    \n",
    "    one_simration = test_data_list\n",
    "        \n",
    "    # 一連のシミュレーションを一つ(ts=0~598までの599個のデータ) 格納用\n",
    "    model_rollout_data_list = [] \n",
    "    \n",
    "    for ts, one_step in enumerate(one_simration):\n",
    "        one_step = one_step.to(device)\n",
    "        with torch.no_grad():\n",
    "            if ts == 0:\n",
    "                pred = test_model(one_step,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge)\n",
    "            else:\n",
    "                pred = test_model(prev_step, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge)\n",
    "            \n",
    "            one_simulation_loss += test_model.one_step_loss(pred, one_step, mean_vec_y,std_vec_y)\n",
    "            \n",
    "            pad_length = 11  # 埋める後の列数\n",
    "            # pad_length = one_step.x.dim()  # 埋める後の列数\n",
    "            new_pred = torch.zeros((pred.size(0), pad_length), device= device)  # 全てゼロの新しいテンソルを作成\n",
    "            # new_pred = torch.zeros((pred.size(0), pad_length), dtype=torch.int, device= device)  # 全てゼロの新しいテンソルを作成\n",
    "            new_pred[:, :2] = pred  # 元のテンソルをコピー\n",
    "            # new_pred[:, :pred.size(1)] = pred  # 元のテンソルをコピー\n",
    "            \n",
    "            displace_x = new_pred * delta_t\n",
    "            if ts == 0:\n",
    "                next_x = one_step.x + displace_x\n",
    "            else:\n",
    "                next_x = prev_step.x + displace_x\n",
    "\n",
    "            data_dict = { #add yamada\n",
    "                'x': next_x,\n",
    "                'edge_index': one_step.edge_index, \n",
    "                'edge_attr': one_step.edge_attr,\n",
    "                'y': pred,\n",
    "                'p': one_step.p,\n",
    "                'cells': one_step.cells,\n",
    "                'mesh_pos': one_step.mesh_pos\n",
    "                } #add yamada\n",
    "            prev_step = Data(\n",
    "                x=data_dict['x'],\n",
    "                edge_index=data_dict['edge_index'],\n",
    "                edge_attr=data_dict['edge_attr'],\n",
    "                y=data_dict['y'],\n",
    "                p=data_dict['p'],\n",
    "                cells=data_dict['cells'],\n",
    "                mesh_pos=data_dict['mesh_pos']\n",
    "            )\n",
    "        \n",
    "            model_rollout_data_list.append(prev_step) # add yamada\n",
    "\n",
    "            #calculate validation error if asked to\n",
    "            if (is_validation):\n",
    "                #Like for the MeshGraphNets model, calculate the mask over which we calculate\n",
    "                #flow loss and add this calculated RMSE value to our val error\n",
    "                normal = torch.tensor(0)\n",
    "                outflow = torch.tensor(5)\n",
    "                loss_mask = torch.logical_or((torch.argmax(one_step.x[:, 2:], dim=1) == torch.tensor(0)),\n",
    "                                            (torch.argmax(one_step.x[:, 2:], dim=1) == torch.tensor(5)))\n",
    "\n",
    "                eval_velo = one_step.x[:, 0:2] + unnormalize( pred[:], mean_vec_y, std_vec_y ) * delta_t\n",
    "                gs_velo = one_step.x[:, 0:2] + one_step.y[:] * delta_t\n",
    "                \n",
    "                error = torch.sum((eval_velo - gs_velo) ** 2, dim=1)\n",
    "                velo_rmse += torch.sqrt(torch.mean(error[loss_mask]))\n",
    "\n",
    "            num_loops+=1 # lossを計算した回数\n",
    "                \n",
    "    # if velocity is evaluated, return velo_rmse as 0\n",
    "    return one_simulation_loss/num_loops, velo_rmse/num_loops, model_rollout_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(args, losses, test_losses, velo_val_losses):\n",
    "    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "\n",
    "    if not os.path.isdir(args.postprocess_dir):\n",
    "        os.mkdir(args.postprocess_dir)\n",
    "\n",
    "    PATH = os.path.join(args.postprocess_dir, model_name + '.pdf')\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.title('Losses Plot')\n",
    "    plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "    plt.plot(test_losses, label=\"test loss\" + \" - \" + args.model_type)\n",
    "    #if (args.save_velo_val):\n",
    "    #    plt.plot(velo_val_losses, label=\"velocity loss\" + \" - \" + args.model_type)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    f.savefig(PATH, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未調整\n",
    "\n",
    "def make_animation(gs, pred, evl, path, name , skip = 2, save_anim = True, plot_variables = False):\n",
    "    '''\n",
    "    input gs is a dataloader and each entry contains attributes of many timesteps.\n",
    "\n",
    "    '''\n",
    "    print('Generating velocity fields...')\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(20, 16))\n",
    "    num_steps = len(gs) # for a single trajectory\n",
    "    num_frames = num_steps // skip\n",
    "    print(num_steps)\n",
    "    def animate(num):\n",
    "        step = (num*skip) % num_steps\n",
    "        traj = 0\n",
    "\n",
    "        bb_min = gs[0].x[:, 0:2].min() # first two columns are velocity\n",
    "        bb_max = gs[0].x[:, 0:2].max() # use max and min velocity of gs dataset at the first step for both \n",
    "                                          # gs and prediction plots\n",
    "        bb_min_evl = evl[0].x[:, 0:2].min()  # first two columns are velocity\n",
    "        bb_max_evl = evl[0].x[:, 0:2].max()  # use max and min velocity of gs dataset at the first step for both \n",
    "                                          # gs and prediction plots\n",
    "        count = 0\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.cla()\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_axis_off()\n",
    "            \n",
    "            pos = gs[step].mesh_pos \n",
    "            faces = gs[step].cells\n",
    "            if (count == 0):\n",
    "                # ground truth\n",
    "                velocity = gs[step].x[:, 0:2]\n",
    "                title = 'Ground truth:'\n",
    "            elif (count == 1):\n",
    "                velocity = pred[step].x[:, 0:2]\n",
    "                title = 'Prediction:'\n",
    "            else: \n",
    "                velocity = evl[step].x[:, 0:2]\n",
    "                title = 'Error: (Prediction - Ground truth)'\n",
    "\n",
    "            triang = mtri.Triangulation(pos[:, 0], pos[:, 1], faces)\n",
    "            if (count <= 1):\n",
    "                # absolute values\n",
    "                \n",
    "                mesh_plot = ax.tripcolor(triang, velocity[:, 0], vmin= bb_min, vmax=bb_max,  shading='flat' ) # x-velocity\n",
    "                ax.triplot(triang, 'ko-', ms=0.5, lw=0.3)\n",
    "            else:\n",
    "                # error: (pred - gs)/gs\n",
    "                mesh_plot = ax.tripcolor(triang, velocity[:, 0], vmin= bb_min_evl, vmax=bb_max_evl, shading='flat' ) # x-velocity\n",
    "                ax.triplot(triang, 'ko-', ms=0.5, lw=0.3)\n",
    "                #ax.triplot(triang, lw=0.5, color='0.5')\n",
    "\n",
    "            ax.set_title('{} Trajectory {} Step {}'.format(title, traj, step), fontsize = '20')\n",
    "            #ax.color\n",
    "\n",
    "            #if (count == 0):\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            clb = fig.colorbar(mesh_plot, cax=cax, orientation='vertical')\n",
    "            clb.ax.tick_params(labelsize=20) \n",
    "            \n",
    "            clb.ax.set_title('x velocity (m/s)',\n",
    "                             fontdict = {'fontsize': 20})\n",
    "            count += 1\n",
    "        return fig,\n",
    "\n",
    "    # Save animation for visualization\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if (save_anim):\n",
    "        gs_anim = animation.FuncAnimation(fig, animate, frames=num_frames, interval=1000)\n",
    "      #change from\n",
    "        # writergif = animation.PillowWriter(fps=10) \n",
    "        # anim_path = os.path.join(path, '{}_anim.gif'.format(name))\n",
    "        # gs_anim.save( anim_path, writer=writergif)\n",
    "        \n",
    "      #change to\n",
    "        anim_path = os.path.join(path, '{}_anim.mp4'.format(name))\n",
    "        gs_anim.save( anim_path, writer='ffmeg', fps=10)\n",
    "        \n",
    "        plt.show(block=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未調整\n",
    "\n",
    "def visualize(best_model, file_dir, args, gif_name, delta_t = 0.01, skip = 1):\n",
    "\n",
    "    best_model.eval()\n",
    "    device = args.device\n",
    "    viz_data = {}\n",
    "    gs_data = {}\n",
    "    eval_data = {}\n",
    "    # viz_data_loader = copy.deepcopy(loader)\n",
    "    # gs_data_loader = copy.deepcopy(loader)\n",
    "    # eval_data_loader = copy.deepcopy(loader)\n",
    "    \n",
    "    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list\n",
    "    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),\n",
    "            std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))\n",
    "    \n",
    "    \n",
    "    # flag = True #add yamada\n",
    "    # prev_pred_x = None\n",
    "    \n",
    "    for data in zip(best_model_rollout_data):\n",
    "        viz_data.x = data.x\n",
    "        data=data.to(args.device) \n",
    "        viz_data = data.to(args.device)\n",
    "        \n",
    "        gs_data.x[:, 0:2] = data.x[:, 0:2] + data.y* delta_t\n",
    "        # gs_data - viz_data = error_data\n",
    "        eval_data.x[:, 0:2] = (viz_data.x[:, 0:2] - gs_data.x[:, 0:2])\n",
    "  \n",
    "    #print(viz_data_loader)\n",
    "    make_animation(gs_data_loader, viz_data_loader, eval_data_loader, file_dir,\n",
    "                      gif_name, skip, True, False)\n",
    "\n",
    "    return eval_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5)  #Torch\n",
    "random.seed(5)        #Python\n",
    "np.random.seed(5)     #NumPy\n",
    "\n",
    "for args in [\n",
    "        {'model_type': 'meshgraphnet',  \n",
    "         'num_layers': 10,\n",
    "         'batch_size': 2, \n",
    "        #  'batch_size': 16, \n",
    "         'hidden_dim': 10, \n",
    "         'epochs': 500,\n",
    "        #  'epochs': 5000,\n",
    "         'opt': 'adam', \n",
    "         'opt_scheduler': 'none', \n",
    "         'opt_restart': 0, \n",
    "         'weight_decay': 5e-4, \n",
    "         'lr': 0.001,\n",
    "        #  'train_size': 3, \n",
    "        #  'test_size': 1, \n",
    "         'train_traj': 4, # 合わせて 　　　今回はtrainはデータ数100に固定した\n",
    "         'test_traj': 2,  # 50以下ならOK\n",
    "         'device':'cuda',\n",
    "         'shuffle': True, \n",
    "         'save_velo_val': True,\n",
    "         'save_best_model': True, \n",
    "         'checkpoint_dir': './best_models/',\n",
    "         'postprocess_dir': './2d_loss_plots/'},\n",
    "    ]:\n",
    "        args = objectview(args)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.device = device\n",
    "print('device is {}'.format(device))\n",
    "\n",
    "# dataset = torch.load(file_path)[:(args.train_size+args.test_size)]\n",
    "if args.train_traj + args.test_traj > len(new_data_lists)*len(new_data_lists[0]):\n",
    "    print(\"(args.train_traj + args.test_traj) is too big.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare dataset(from new_data_lists)\n",
    "actual_train_dataset = []\n",
    "\n",
    "acutual_train_data_lists = new_data_lists[0:args.train_traj]\n",
    "for acutual_train_data_list in acutual_train_data_lists:\n",
    "    for acutual_train_data in acutual_train_data_list:\n",
    "        actual_train_dataset.append(acutual_train_data)\n",
    "\n",
    "actual_train_dataset = actual_train_dataset[:100]\n",
    "\n",
    "acutual_test_data_lists = new_data_lists[args.train_traj:(args.train_traj + args.test_traj)]\n",
    "\n",
    "if(args.shuffle):\n",
    "  random.shuffle(actual_train_dataset)\n",
    "\n",
    "train_stats_list = get_stats(actual_train_dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train!(get best model)\n",
    "test_losses, losses, velo_val_losses, best_model, best_test_loss = train(actual_train_dataset, acutual_test_data_lists, device, train_stats_list, args)\n",
    "\n",
    "print(\"Min test set loss: {0}\".format(min(test_losses)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "if (args.save_velo_val):\n",
    "    print(\"Minimum velocity validation loss: {0}\".format(min(velo_val_losses)))\n",
    "    \n",
    "save_plots(args, losses, test_losses, velo_val_losses)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
